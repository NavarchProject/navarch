# Example pool configuration for Navarch
#
# This file shows how to configure GPU node pools with scaling limits,
# autoscaler strategies, and health policies.

pools:
  # Training pool: Reactive scaling based on utilization
  - name: training
    provider: lambda
    instance_type: gpu_8x_h100_sxm5
    region: us-west-2
    
    scaling:
      min_nodes: 2
      max_nodes: 20
      cooldown_period: 5m
      
      # Reactive autoscaler: scale based on current utilization
      autoscaler:
        type: reactive
        scale_up_threshold: 80
        scale_down_threshold: 20
    
    health:
      unhealthy_threshold: 2
      auto_replace: true
    
    labels:
      workload: training
      gpu-memory: "80GB"

  # Inference pool: Queue-based scaling for request handling
  - name: inference
    provider: lambda
    instance_type: gpu_1x_a100_sxm4
    region: us-east-1
    
    scaling:
      min_nodes: 4
      max_nodes: 50
      cooldown_period: 2m
      
      # Queue-based autoscaler: scale based on pending jobs
      autoscaler:
        type: queue
        jobs_per_node: 100
    
    health:
      unhealthy_threshold: 3
      auto_replace: true
    
    labels:
      workload: inference

  # Batch pool: Scheduled scaling for known patterns
  - name: batch
    provider: lambda
    instance_type: gpu_1x_a100_80gb_sxm4
    region: us-west-2
    
    scaling:
      min_nodes: 0
      max_nodes: 100
      cooldown_period: 3m
      
      # Scheduled autoscaler: different limits by time
      autoscaler:
        type: scheduled
        schedule:
          # Business hours: larger capacity
          - days: [monday, tuesday, wednesday, thursday, friday]
            start_hour: 9
            end_hour: 18
            min_nodes: 10
            max_nodes: 100
          # Off hours: minimal capacity
          - days: [saturday, sunday]
            start_hour: 0
            end_hour: 24
            min_nodes: 0
            max_nodes: 10
        # Fall back to reactive when not in scheduled window
        fallback:
          type: reactive
          scale_up_threshold: 80
          scale_down_threshold: 20
    
    health:
      unhealthy_threshold: 2
      auto_replace: true
    
    labels:
      workload: batch

  # ML training pool: Predictive scaling anticipates demand
  - name: ml-training
    provider: gcp
    instance_type: a3-highgpu-8g
    region: us-central1
    
    scaling:
      min_nodes: 2
      max_nodes: 50
      cooldown_period: 10m
      
      # Predictive autoscaler: uses historical data to forecast
      autoscaler:
        type: predictive
        lookback_window: 30        # Use last 30 samples
        growth_factor: 1.5         # Scale 1.5x predicted need
        fallback:
          type: reactive
          scale_up_threshold: 70
          scale_down_threshold: 30
    
    health:
      unhealthy_threshold: 2
      auto_replace: true
    
    labels:
      workload: ml-training

  # Critical pool: Composite scaling for high availability
  - name: critical
    provider: lambda
    instance_type: gpu_8x_h100_sxm5
    region: us-west-2
    
    scaling:
      min_nodes: 5
      max_nodes: 30
      cooldown_period: 2m
      
      # Composite autoscaler: combines multiple strategies
      autoscaler:
        type: composite
        mode: max              # Take the highest recommendation
        autoscalers:
          - type: reactive
            scale_up_threshold: 70
            scale_down_threshold: 30
          - type: queue
            jobs_per_node: 50
    
    health:
      unhealthy_threshold: 1   # Replace immediately on failure
      auto_replace: true
    
    labels:
      workload: critical
      sla: high

# Global settings
global:
  # Default SSH keys for all pools
  ssh_key_names:
    - ops-team
    - ml-team
  
  # Agent configuration
  agent:
    server: https://control-plane.example.com
    heartbeat_interval: 30s
    health_check_interval: 60s
  
  # Cost controls
  cost:
    # Maximum monthly spend across all pools
    max_monthly_spend_usd: 50000
    
    # Alert thresholds
    alert_at_percent: [50, 75, 90]

# Provider credentials (reference secrets, do not inline)
providers:
  lambda:
    api_key_secret: navarch/lambda-api-key  # From secret manager
  
  gcp:
    project: my-gcp-project
    credentials_secret: navarch/gcp-credentials
  
  aws:
    region: us-east-1
    credentials_secret: navarch/aws-credentials

