{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Home","text":"Navarch <p>Open-source GPU fleet management</p> <p>Navarch automates provisioning, health monitoring, and lifecycle management of GPU nodes across cloud providers.</p> Get Started Learn Concepts GitHub \u26a0\ufe0f Experimental \u2014 not production-ready <ul> <li> <p> Health Monitoring</p> <p>Detect GPU failures in real time. Catches XID errors, thermal issues, ECC faults, and NVLink failures via NVML before they crash your workloads.</p> </li> <li> <p> Auto-Replacement</p> <p>Unhealthy nodes get terminated and replaced automatically. Define health policies with CEL expressions. Your pool stays at capacity.</p> </li> <li> <p> Multi-Cloud</p> <p>Provision across Lambda Labs, GCP, and AWS from a single config. Failover between providers or optimize for cost.</p> </li> <li> <p> Autoscaling</p> <p>Scale based on GPU utilization, queue depth, schedules, or predictions. Cooldown prevents thrashing. Combine multiple strategies.</p> </li> <li> <p> Pool Management</p> <p>Group nodes by instance type, region, or workload. Set scaling limits, health policies, and labels per pool.</p> </li> <li> <p> Simulator</p> <p>Test policies and failure scenarios locally. Stress test with 1000+ simulated nodes before deploying to production.</p> </li> </ul>"},{"location":"#why-navarch","title":"Why Navarch","text":"<p>GPUs fail. Cloud providers give you instances, but detecting hardware failures and replacing bad nodes is your problem. Teams end up building custom monitoring with DCGM, dmesg parsing, and cloud-specific scripts. Then there's the multi-cloud problem: different APIs, different instance types, different tooling.</p> <p>Navarch makes your GPU supply self-healing and fungible across clouds, all under one system to manage it all:</p> <ul> <li>Unified health monitoring for XID errors, thermal events, ECC faults, and NVLink</li> <li>Automatic replacement when nodes fail health checks</li> <li>Source GPUs anywhere. Lambda out of H100s? Failover to GCP or AWS automatically.</li> <li>Single control plane for Lambda, GCP, and AWS. One config, one API.</li> <li>Works with your scheduler. Kubernetes, SLURM, or bare metal.</li> </ul>"},{"location":"#how-it-works","title":"How it works","text":"<p>The control plane manages pools, evaluates health policies, and provisions or terminates instances through cloud provider APIs.</p> <p>The node agent runs on each GPU instance. It reports health via NVML, sends heartbeats, and executes commands from the control plane.</p> <p>Navarch complements your existing scheduler. It handles infrastructure; your scheduler places workloads.</p>"},{"location":"#quick-look","title":"Quick look","text":"<pre><code># navarch.yaml\nproviders:\n  lambda:\n    type: lambda\n    api_key_env: LAMBDA_API_KEY\n\npools:\n  training:\n    provider: lambda\n    instance_type: gpu_8x_h100_sxm5\n    region: us-west-1\n    min_nodes: 2\n    max_nodes: 8\n    health:\n      auto_replace: true\n    autoscaling:\n      type: reactive\n      scale_up_at: 80\n      scale_down_at: 20\n</code></pre> <pre><code>control-plane -config navarch.yaml\n</code></pre>"},{"location":"#next-steps","title":"Next steps","text":"<ul> <li> <p>Getting Started</p> <p>Set up Navarch with Lambda Labs.</p> <p> Getting started</p> </li> <li> <p>Core Concepts</p> <p>Pools, providers, health checks, node lifecycle.</p> <p> Concepts</p> </li> <li> <p>Configuration</p> <p>Full reference for navarch.yaml.</p> <p> Configuration</p> </li> <li> <p>Architecture</p> <p>How Navarch integrates with your stack.</p> <p> Architecture</p> </li> </ul>"},{"location":"architecture/","title":"Architecture","text":"<p>Navarch is an infrastructure layer that sits between cloud providers and workload schedulers.</p>"},{"location":"architecture/#system-layers","title":"System layers","text":"<pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 Workload Schedulers                        \u2502\n\u2502 (Kubernetes, Slurm, Ray, custom)           \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                    \u2193 schedule jobs\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 Navarch                                    \u2502\n\u2502 - Provisions GPU VMs                       \u2502\n\u2502 - Monitors hardware health                 \u2502\n\u2502 - Autoscales node pools                    \u2502\n\u2502 - Auto-replaces failures                   \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                    \u2193 provision/terminate\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 Cloud Provider APIs                        \u2502\n\u2502 (Lambda Labs, GCP, AWS)                    \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>Your scheduler places workloads. Navarch maintains healthy infrastructure.</p>"},{"location":"architecture/#components","title":"Components","text":"<p>See Components for details.</p> <p>Control plane: gRPC server that manages pools, tracks node state, and issues commands.</p> <p>Node agent: Lightweight process on each GPU instance that reports health and executes commands.</p> <p>Pool manager: Orchestrates autoscaling and node replacement.</p>"},{"location":"architecture/#deployment-models","title":"Deployment models","text":""},{"location":"architecture/#single-control-plane","title":"Single control plane","text":"<p>One control plane for all pools. Suitable for most deployments.</p> <pre><code>control-plane --config navarch.yaml\n</code></pre>"},{"location":"architecture/#high-availability","title":"High availability","text":"<p>Multiple control planes behind a load balancer with external state store. See Deployment for details.</p>"},{"location":"architecture/#multi-region","title":"Multi-region","text":"<p>Separate control planes per region with independent configurations. Use for latency-sensitive workloads or regulatory requirements.</p>"},{"location":"architecture/#learn-more","title":"Learn more","text":"<ul> <li>Components - Control plane and node agent details</li> <li>Pools &amp; Providers - Multi-cloud provisioning</li> <li>Health Monitoring - GPU failure detection</li> <li>Autoscaling - Scaling strategies</li> <li>Extending - Custom providers, autoscalers, metrics sources</li> </ul>"},{"location":"authentication/","title":"Authentication","text":"<p>Navarch supports pluggable authentication for the control plane API. This document covers the built-in bearer token authentication and how to implement custom authentication methods.</p>"},{"location":"authentication/#bearer-token-authentication","title":"Bearer token authentication","text":"<p>The control plane includes built-in support for bearer token authentication. When enabled, all API requests (except health endpoints) require a valid token in the <code>Authorization</code> header.</p>"},{"location":"authentication/#enabling-authentication","title":"Enabling authentication","text":"<p>To enable authentication on the control plane:</p> <pre><code># Using environment variable\nexport NAVARCH_AUTH_TOKEN=\"your-secret-token\"\ncontrol-plane --config config.yaml\n\n# Using command-line flag\ncontrol-plane --auth-token \"your-secret-token\"\n</code></pre> <p>The environment variable takes precedence if both are set.</p>"},{"location":"authentication/#exempt-endpoints","title":"Exempt endpoints","text":"<p>The following endpoints do not require authentication:</p> <ul> <li><code>/healthz</code> \u2014 Liveness probe for orchestrators.</li> <li><code>/readyz</code> \u2014 Readiness probe for load balancers.</li> <li><code>/metrics</code> \u2014 Prometheus metrics endpoint.</li> </ul>"},{"location":"authentication/#client-configuration","title":"Client configuration","text":"<p>Clients must include the token in the <code>Authorization</code> header using the Bearer scheme.</p> <p>For the CLI:</p> <pre><code>export NAVARCH_AUTH_TOKEN=\"your-secret-token\"\nnavarch list\n</code></pre> <p>For curl:</p> <pre><code>curl -H \"Authorization: Bearer your-secret-token\" \\\n  http://localhost:50051/navarch.ControlPlaneService/ListNodes\n</code></pre> <p>For node agents:</p> <pre><code>export NAVARCH_AUTH_TOKEN=\"your-secret-token\"\nnode-agent --server https://control-plane.example.com\n</code></pre>"},{"location":"authentication/#token-generation","title":"Token generation","text":"<p>Generate a secure token using a cryptographically secure random source:</p> <pre><code># Using openssl\nopenssl rand -base64 32\n\n# Using /dev/urandom\nhead -c 32 /dev/urandom | base64\n</code></pre> <p>Store tokens securely using your cloud provider's secret manager (AWS Secrets Manager, GCP Secret Manager, HashiCorp Vault).</p>"},{"location":"authentication/#custom-authentication","title":"Custom authentication","text":"<p>For authentication methods beyond bearer tokens (JWT, OIDC, mTLS), implement the <code>Authenticator</code> interface and rebuild the control plane.</p>"},{"location":"authentication/#authenticator-interface","title":"Authenticator interface","text":"<pre><code>type Authenticator interface {\n    AuthenticateRequest(r *http.Request) (*Identity, bool, error)\n}\n</code></pre> <p>Return values:</p> <ul> <li><code>(*Identity, true, nil)</code> \u2014 Authentication succeeded.</li> <li><code>(nil, false, nil)</code> \u2014 No credentials found; try the next authenticator.</li> <li><code>(nil, false, error)</code> \u2014 Credentials found but invalid.</li> </ul>"},{"location":"authentication/#identity","title":"Identity","text":"<p>The <code>Identity</code> struct represents an authenticated entity:</p> <pre><code>type Identity struct {\n    Subject string              // Primary identifier (e.g., \"user:jane@example.com\")\n    Groups  []string            // Group memberships for authorization\n    Extra   map[string][]string // Additional claims from the auth source\n}\n</code></pre> <p>The <code>Subject</code> field aligns with the JWT <code>sub</code> claim and X.509 certificate subject.</p>"},{"location":"authentication/#chaining-authenticators","title":"Chaining authenticators","text":"<p>Use <code>ChainAuthenticator</code> to try multiple authentication methods in sequence:</p> <pre><code>chain := auth.NewChainAuthenticator(\n    jwtAuthenticator,     // Try JWT first\n    bearerAuthenticator,  // Fall back to static token\n)\n\nmiddleware := auth.NewMiddleware(chain,\n    auth.WithExcludedPaths(\"/healthz\", \"/readyz\", \"/metrics\"),\n)\n</code></pre> <p>The first authenticator to return success wins. If an authenticator returns an error (invalid credentials), the chain stops and returns that error.</p>"},{"location":"authentication/#example-jwt-authenticator","title":"Example: JWT authenticator","text":"<pre><code>type JWTAuthenticator struct {\n    keyFunc jwt.Keyfunc\n    issuer  string\n}\n\nfunc (a *JWTAuthenticator) AuthenticateRequest(r *http.Request) (*auth.Identity, bool, error) {\n    header := r.Header.Get(\"Authorization\")\n    if !strings.HasPrefix(header, \"Bearer \") {\n        return nil, false, nil // No JWT, try next authenticator\n    }\n\n    tokenString := strings.TrimPrefix(header, \"Bearer \")\n    token, err := jwt.Parse(tokenString, a.keyFunc)\n    if err != nil {\n        return nil, false, fmt.Errorf(\"invalid token: %w\", err)\n    }\n\n    claims := token.Claims.(jwt.MapClaims)\n    return &amp;auth.Identity{\n        Subject: claims[\"sub\"].(string),\n        Groups:  extractGroups(claims),\n    }, true, nil\n}\n</code></pre>"},{"location":"authentication/#security-considerations","title":"Security considerations","text":"<p>When implementing custom authenticators:</p> <ul> <li>Use <code>crypto/subtle.ConstantTimeCompare</code> for secret comparison to prevent timing attacks.</li> <li>Return generic error messages (\"unauthorized\") to avoid leaking why authentication failed.</li> <li>Copy slices and maps in the returned <code>Identity</code> to prevent mutation between requests.</li> <li>Validate token expiration, issuer, and audience claims for JWT/OIDC.</li> </ul>"},{"location":"authentication/#retrieving-identity-in-handlers","title":"Retrieving identity in handlers","text":"<p>After authentication, the <code>Identity</code> is available in the request context:</p> <pre><code>func handleRequest(w http.ResponseWriter, r *http.Request) {\n    identity := auth.IdentityFromContext(r.Context())\n    if identity == nil {\n        // Unauthenticated request (only possible if WithRequireAuth(false))\n        return\n    }\n\n    log.Printf(\"Request from %s\", identity.Subject)\n}\n</code></pre>"},{"location":"authentication/#what-is-next","title":"What is next","text":"<ul> <li>Configuration reference \u2014 Server and pool configuration.</li> <li>Deployment guide \u2014 Production deployment with TLS.</li> </ul>"},{"location":"bootstrap/","title":"Node Bootstrap","text":"<p>Navarch can run setup commands on newly provisioned instances via SSH. This is useful for installing the node agent, configuring GPU drivers, or running custom initialization scripts.</p>"},{"location":"bootstrap/#configuration","title":"Configuration","text":"<pre><code>pools:\n  training:\n    provider: lambda\n    instance_type: gpu_8x_h100_sxm5\n    min_nodes: 2\n    max_nodes: 20\n    ssh_user: ubuntu\n    ssh_private_key_path: ~/.ssh/navarch-key\n    setup_commands:\n      - |\n        curl -L https://github.com/NavarchProject/navarch/releases/latest/download/navarch-node-linux-amd64 \\\n          -o /usr/local/bin/navarch-node &amp;&amp; chmod +x /usr/local/bin/navarch-node\n      - |\n        navarch-node --server {{.ControlPlane}} --node-id {{.NodeID}} &amp;\n</code></pre>"},{"location":"bootstrap/#bootstrap-fields","title":"Bootstrap fields","text":"Field Required Description <code>setup_commands</code> No List of shell commands to run on the node after provisioning <code>ssh_user</code> No SSH username (default: <code>ubuntu</code>) <code>ssh_private_key_path</code> Yes* Path to SSH private key file <code>ip_wait_timeout</code> No Max time to wait for instance IP (default: <code>15m</code>) <code>ssh_timeout</code> No Max time to wait for SSH to become available (default: <code>10m</code>) <code>ssh_connect_timeout</code> No Timeout for each SSH connection attempt (default: <code>30s</code>) <code>command_timeout</code> No Max time for each command to execute (default: <code>5m</code>) <p>*Required when <code>setup_commands</code> is specified.</p> <p>These fields can also be set in <code>defaults</code> to apply to all pools:</p> <pre><code>defaults:\n  ssh_user: ubuntu\n  ssh_private_key_path: ~/.ssh/navarch-key\n</code></pre>"},{"location":"bootstrap/#template-variables","title":"Template variables","text":"<p>Setup commands support Go template syntax. The following variables are available:</p> Variable Description Example <code>{{.ControlPlane}}</code> Control plane URL <code>http://control-plane.example.com:50051</code> <code>{{.Pool}}</code> Pool name <code>training</code> <code>{{.NodeID}}</code> Unique node identifier <code>node-abc123</code> <code>{{.Provider}}</code> Provider name <code>lambda</code> <code>{{.Region}}</code> Region where node is provisioned <code>us-west-2</code> <code>{{.InstanceType}}</code> Instance type <code>gpu_8x_h100_sxm5</code>"},{"location":"bootstrap/#how-it-works","title":"How it works","text":"<ol> <li>When Navarch provisions a new instance, it waits for the instance to receive an IP address.</li> <li>Once the IP is available, it waits for SSH to become available.</li> <li>Once connected, it runs each setup command in order, enforcing the command timeout.</li> <li>If any command fails or times out, the bootstrap is aborted and the node is marked as failed.</li> <li>On success, the node is ready to receive workloads.</li> </ol> <p>Commands that exceed <code>command_timeout</code> receive a <code>SIGKILL</code> signal on the remote host.</p>"},{"location":"bootstrap/#timeouts","title":"Timeouts","text":"<p>Configure timeouts based on your infrastructure:</p> <pre><code>pools:\n  training:\n    provider: lambda\n    instance_type: gpu_8x_h100_sxm5\n    setup_commands:\n      - ./long-running-setup.sh\n\n    # Fast-booting instances with long setup scripts\n    ip_wait_timeout: 5m\n    ssh_timeout: 3m\n    command_timeout: 30m\n</code></pre> Timeout Default Use case <code>ip_wait_timeout</code> 15m Increase for slow cloud providers or complex networking <code>ssh_timeout</code> 10m Decrease for pre-configured images with fast boot <code>ssh_connect_timeout</code> 30s Increase for high-latency networks <code>command_timeout</code> 5m Increase for large downloads or compilations <p>The control plane logs detailed information about each bootstrap phase:</p> <ul> <li>SSH connection attempts and timing</li> <li>Each command executed with duration</li> <li>stdout/stderr output on failure</li> <li>Total bootstrap duration</li> </ul>"},{"location":"bootstrap/#example-full-node-setup","title":"Example: Full node setup","text":"<pre><code>defaults:\n  ssh_user: ubuntu\n  ssh_private_key_path: ~/.ssh/navarch-key\n\npools:\n  training:\n    provider: lambda\n    instance_type: gpu_8x_h100_sxm5\n    min_nodes: 2\n    max_nodes: 20\n    setup_commands:\n      # Install NVIDIA drivers if not present\n      - |\n        if ! command -v nvidia-smi &amp;&gt; /dev/null; then\n          apt-get update &amp;&amp; apt-get install -y nvidia-driver-535\n        fi\n      # Download and install the node agent\n      - |\n        curl -L https://github.com/NavarchProject/navarch/releases/latest/download/navarch-node-linux-amd64 \\\n          -o /usr/local/bin/navarch-node\n        chmod +x /usr/local/bin/navarch-node\n      # Create systemd service\n      - |\n        cat &gt; /etc/systemd/system/navarch-node.service &lt;&lt; EOF\n        [Unit]\n        Description=Navarch Node Agent\n        After=network.target\n\n        [Service]\n        ExecStart=/usr/local/bin/navarch-node --server {{.ControlPlane}} --node-id {{.NodeID}} --pool {{.Pool}}\n        Restart=always\n        RestartSec=10\n\n        [Install]\n        WantedBy=multi-user.target\n        EOF\n      # Start the agent\n      - systemctl daemon-reload &amp;&amp; systemctl enable navarch-node &amp;&amp; systemctl start navarch-node\n</code></pre>"},{"location":"bootstrap/#comparison-with-other-deployment-methods","title":"Comparison with other deployment methods","text":"Method Use case SSH bootstrap Control plane manages agent installation. Good for managed fleets. Custom images Pre-bake agent into AMI/image. Fastest startup. Cloud-init Provider runs script at boot. No SSH needed. Container Run agent as Docker/K8s workload. <p>See Deployment for details on each approach.</p>"},{"location":"cli/","title":"Navarch CLI reference","text":"<p>The Navarch CLI is a command-line tool for managing your GPU fleet across cloud providers.</p>"},{"location":"cli/#installation","title":"Installation","text":"<pre><code># From source\ngit clone https://github.com/NavarchProject/navarch.git\ncd navarch\nmake build\nsudo cp bin/navarch /usr/local/bin/\n\n# Or using Go\ngo install github.com/NavarchProject/navarch/cmd/navarch@latest\n</code></pre>"},{"location":"cli/#configuration","title":"Configuration","text":"<p>The CLI communicates with the Navarch control plane via HTTP. You can configure the control plane address using any of these methods, in order of precedence:</p> <ol> <li>Command-line flag (highest priority): <code>--server</code> or <code>-s</code></li> <li>Environment variable: <code>NAVARCH_SERVER</code></li> <li>Default value (lowest priority): <code>http://localhost:50051</code></li> </ol>"},{"location":"cli/#global-flags","title":"Global flags","text":"<p>All commands support these flags:</p> <pre><code>-s, --server string      Control plane address (default \"http://localhost:50051\")\n--insecure               Skip TLS certificate verification\n-o, --output string      Output format: table, json (default \"table\")\n--timeout duration       Request timeout (default 30s)\n-h, --help               Show help for any command\n</code></pre>"},{"location":"cli/#examples","title":"Examples","text":"<p>Connect to a remote control plane using the flag:</p> <pre><code>navarch -s https://navarch.example.com list\n</code></pre> <p>Set the control plane address using an environment variable:</p> <pre><code>export NAVARCH_SERVER=https://navarch.example.com\nnavarch list\n</code></pre> <p>Override the environment variable with a flag:</p> <pre><code>export NAVARCH_SERVER=https://prod.example.com\nnavarch -s https://staging.example.com list  # Uses staging\n</code></pre> <p>Get JSON output for scripting:</p> <pre><code>navarch list -o json | jq '.[] | select(.status == \"ACTIVE\")'\n</code></pre>"},{"location":"cli/#commands","title":"Commands","text":""},{"location":"cli/#navarch-list","title":"<code>navarch list</code>","text":"<p>List all nodes in your fleet.</p> <p>Usage:</p> <pre><code>navarch list [flags]\n</code></pre> <p>Flags:</p> <pre><code>--provider string   Filter by cloud provider (gcp, aws, azure)\n--region string     Filter by region (us-central1, us-east-1, etc.)\n--status string     Filter by status (active, cordoned, draining, terminated)\n</code></pre> <p>Examples:</p> <p>To list all nodes: <pre><code>$ navarch list\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 Node ID     \u2502 Provider \u2502 Region      \u2502 Zone          \u2502 Instance Type \u2502 Status \u2502 Health  \u2502 Last Heartbeat \u2502 GPUs \u2502\n\u2502 node-gcp-1  \u2502 gcp      \u2502 us-central1 \u2502 us-central1-a \u2502 a3-highgpu-8g \u2502 Active \u2502 Healthy \u2502 30s ago        \u2502 8    \u2502\n\u2502 node-gcp-2  \u2502 gcp      \u2502 us-west1    \u2502 us-west1-b    \u2502 a3-highgpu-8g \u2502 Active \u2502 Healthy \u2502 45s ago        \u2502 8    \u2502\n\u2502 node-aws-1  \u2502 aws      \u2502 us-east-1   \u2502 us-east-1a    \u2502 p5.48xlarge   \u2502 Active \u2502 Healthy \u2502 1m ago         \u2502 8    \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre></p> <p>To filter by provider: <pre><code>$ navarch list --provider gcp\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 Node ID     \u2502 Provider \u2502 Region      \u2502 Zone          \u2502 Instance Type \u2502 Status \u2502 Health  \u2502 Last Heartbeat \u2502 GPUs \u2502\n\u2502 node-gcp-1  \u2502 gcp      \u2502 us-central1 \u2502 us-central1-a \u2502 a3-highgpu-8g \u2502 Active \u2502 Healthy \u2502 30s ago        \u2502 8    \u2502\n\u2502 node-gcp-2  \u2502 gcp      \u2502 us-west1    \u2502 us-west1-b    \u2502 a3-highgpu-8g \u2502 Active \u2502 Healthy \u2502 45s ago        \u2502 8    \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre></p> <p>To filter by region:</p> <pre><code>$ navarch list --region us-central1\n</code></pre> <p>To get JSON output: <pre><code>$ navarch list -o json\n[\n  {\n    \"node_id\": \"node-gcp-1\",\n    \"provider\": \"gcp\",\n    \"region\": \"us-central1\",\n    \"zone\": \"us-central1-a\",\n    \"instance_type\": \"a3-highgpu-8g\",\n    \"status\": \"NODE_STATUS_ACTIVE\",\n    \"health_status\": \"HEALTH_STATUS_HEALTHY\",\n    \"last_heartbeat\": \"2026-01-19T14:00:00Z\",\n    \"gpus\": [...]\n  }\n]\n</code></pre></p> <p>To combine filters: <pre><code>$ navarch list --provider gcp --region us-central1 --status NODE_STATUS_ACTIVE\n</code></pre></p>"},{"location":"cli/#navarch-get","title":"<code>navarch get</code>","text":"<p>Returns detailed information about a specific node.</p> <p>Usage:</p> <pre><code>navarch get &lt;node-id&gt; [flags]\n</code></pre> <p>Examples:</p> <p>To get node details: <pre><code>$ navarch get node-gcp-1\nNode ID:       node-gcp-1\nProvider:      gcp\nRegion:        us-central1\nZone:          us-central1-a\nInstance Type: a3-highgpu-8g\nStatus:        Active\nHealth:        Healthy\nLast Heartbeat: 30s ago\n\nGPUs:\n  GPU 0:\n    UUID:       GPU-12345678-1234-1234-1234-123456789abc\n    Name:       NVIDIA H100 80GB HBM3\n    PCI Bus ID: 0000:00:04.0\n  GPU 1:\n    UUID:       GPU-87654321-4321-4321-4321-cba987654321\n    Name:       NVIDIA H100 80GB HBM3\n    PCI Bus ID: 0000:00:05.0\n  ... (6 more GPUs)\n\nMetadata:\n  Hostname:    node-gcp-1.c.project.internal\n  Internal IP: 10.128.0.2\n  External IP: 34.123.45.67\n</code></pre></p> <p>To get JSON output:</p> <pre><code>$ navarch get node-gcp-1 -o json\n{\n  \"node_id\": \"node-gcp-1\",\n  \"provider\": \"gcp\",\n  \"region\": \"us-central1\",\n  \"zone\": \"us-central1-a\",\n  \"instance_type\": \"a3-highgpu-8g\",\n  \"status\": \"NODE_STATUS_ACTIVE\",\n  \"health_status\": \"HEALTH_STATUS_HEALTHY\",\n  \"last_heartbeat\": \"2026-01-19T14:00:00Z\",\n  \"gpus\": [...],\n  \"metadata\": {...}\n}\n</code></pre>"},{"location":"cli/#navarch-cordon","title":"<code>navarch cordon</code>","text":"<p>Marks a node as unschedulable. This prevents new workloads from being scheduled on the node but does not affect existing workloads.</p> <p>Usage:</p> <pre><code>navarch cordon &lt;node-id&gt;\n</code></pre> <p>Examples:</p> <p>To cordon a node: <pre><code>$ navarch cordon node-gcp-1\nNode node-gcp-1 cordoned successfully\nCommand ID: a1b2c3d4-e5f6-7890-abcd-ef1234567890\n</code></pre></p> <p>To verify the node is cordoned: <pre><code>$ navarch get node-gcp-1\nNode ID:       node-gcp-1\nProvider:      gcp\nRegion:        us-central1\nZone:          us-central1-a\nInstance Type: a3-highgpu-8g\nStatus:        Cordoned\nHealth:        Healthy\nLast Heartbeat: 1m ago\n</code></pre></p> <p>When to use this command:</p> <ul> <li>Before you perform maintenance on a node.</li> <li>When you suspect a node may have issues but want to observe it.</li> <li>To prevent scheduling on a node without disrupting running workloads.</li> </ul>"},{"location":"cli/#navarch-drain","title":"<code>navarch drain</code>","text":"<p>Drains a node by evicting workloads and marking it unschedulable. This is a more forceful operation than cordoning.</p> <p>Usage:</p> <pre><code>navarch drain &lt;node-id&gt;\n</code></pre> <p>Examples:</p> <p>To drain a node: <pre><code>$ navarch drain node-gcp-1\nNode node-gcp-1 draining\nCommand ID: b2c3d4e5-f6a7-8901-bcde-f12345678901\n</code></pre></p> <p>When to use this command:</p> <ul> <li>Before you decommission a node.</li> <li>When a node is unhealthy and workloads need to be moved.</li> <li>For planned downtime or upgrades.</li> </ul> <p>The drain operation performs the following steps: 1. Marks the node as unschedulable (like cordon). 2. Evicts all running workloads. 3. Transitions the node to <code>DRAINING</code> status.</p>"},{"location":"cli/#navarch-uncordon","title":"<code>navarch uncordon</code>","text":"<p>Marks a cordoned node as schedulable again. This reverses the effect of <code>cordon</code>.</p> <p>Usage:</p> <pre><code>navarch uncordon &lt;node-id&gt;\n</code></pre> <p>Examples:</p> <p>To uncordon a node: <pre><code>$ navarch uncordon node-gcp-1\nNode node-gcp-1 uncordoned successfully\nCommand ID: c3d4e5f6-a7b8-9012-cdef-234567890123\n</code></pre></p> <p>To verify the node is schedulable again: <pre><code>$ navarch get node-gcp-1\nNode ID:       node-gcp-1\nProvider:      gcp\nRegion:        us-central1\nZone:          us-central1-a\nInstance Type: a3-highgpu-8g\nStatus:        Active\nHealth:        Healthy\nLast Heartbeat: 30s ago\n</code></pre></p> <p>When to use this command:</p> <ul> <li>After maintenance is complete and the node is ready for workloads.</li> <li>To bring a previously cordoned node back into service.</li> </ul> <p>Note: You can only uncordon a node that is currently in the <code>Cordoned</code> status. Attempting to uncordon a node in any other status (Active, Draining, Unhealthy, Terminated) will result in an error.</p>"},{"location":"cli/#common-workflows","title":"Common workflows","text":""},{"location":"cli/#monitor-fleet-health","title":"Monitor fleet health","text":"<p>To check all nodes and their health status:</p> <pre><code>$ navarch list\n</code></pre> <p>To filter for unhealthy nodes: <pre><code>$ navarch list -o json | jq '.[] | select(.health_status != \"HEALTH_STATUS_HEALTHY\")'\n</code></pre></p>"},{"location":"cli/#perform-maintenance","title":"Perform maintenance","text":"<ol> <li> <p>Cordon the node to prevent new work:    <pre><code>navarch cordon node-gcp-1\n</code></pre></p> </li> <li> <p>Verify that no new workloads are being scheduled. Check your workload scheduler.</p> </li> <li> <p>Perform maintenance on the node.</p> </li> <li> <p>When ready, uncordon the node:</p> </li> </ol> <pre><code>navarch uncordon node-gcp-1\n</code></pre>"},{"location":"cli/#decommission-a-node","title":"Decommission a node","text":"<ol> <li> <p>Drain the node to evict workloads:    <pre><code>navarch drain node-gcp-1\n</code></pre></p> </li> <li> <p>Wait for workloads to evacuate. Check your workload scheduler.</p> </li> <li> <p>Terminate the node through your cloud provider, or let Navarch handle the termination.</p> </li> </ol>"},{"location":"cli/#investigate-a-problematic-node","title":"Investigate a problematic node","text":"<ol> <li>Get detailed information:</li> </ol> <pre><code>navarch get node-gcp-1\n</code></pre> <ol> <li> <p>Check the GPU details and health status.</p> </li> <li> <p>Decide whether to cordon, drain, or leave the node as-is.</p> </li> </ol>"},{"location":"cli/#scripting-and-automation","title":"Scripting and automation","text":"<p>To count active nodes per region: <pre><code>navarch list -o json | jq 'group_by(.region) | map({region: .[0].region, count: length})'\n</code></pre></p> <p>To get all node IDs in a specific region:</p> <pre><code>navarch list --region us-central1 -o json | jq -r '.[].node_id'\n</code></pre> <p>To check if any nodes have been offline for over 5 minutes:</p> <pre><code>navarch list -o json | jq '.[] | select(.last_heartbeat &lt; (now - 300))'\n</code></pre> <p>To cordon all nodes in a specific zone: <pre><code>for node in $(navarch list --region us-central1 -o json | jq -r '.[] | select(.zone == \"us-central1-a\") | .node_id'); do\n  navarch cordon $node\ndone\n</code></pre></p>"},{"location":"cli/#output-formats","title":"Output formats","text":""},{"location":"cli/#table-default","title":"Table (default)","text":"<p>The table format provides a human-readable table with aligned columns for interactive use.</p> <pre><code>navarch list\n</code></pre>"},{"location":"cli/#json","title":"JSON","text":"<p>The JSON format provides machine-readable output for scripting and automation.</p> <pre><code>navarch list -o json\n</code></pre> <p>You can combine JSON output with <code>jq</code> for filtering: <pre><code># Get all active GCP nodes\nnavarch list -o json | jq '.[] | select(.provider == \"gcp\" and .status == \"NODE_STATUS_ACTIVE\")'\n\n# Count nodes by status\nnavarch list -o json | jq 'group_by(.status) | map({status: .[0].status, count: length})'\n\n# Get nodes with more than 4 GPUs\nnavarch list -o json | jq '.[] | select((.gpus | length) &gt; 4)'\n</code></pre></p>"},{"location":"cli/#exit-codes","title":"Exit codes","text":"<ul> <li><code>0</code> - Success.</li> <li><code>1</code> - General error, such as connection failed or command failed.</li> </ul>"},{"location":"cli/#troubleshooting","title":"Troubleshooting","text":""},{"location":"cli/#connection-refused","title":"Connection refused","text":"<p>Error message: <code>failed to list nodes: connection refused</code></p> <p>To resolve this issue, verify that the control plane is running: <pre><code># Check if control plane is running\ncurl http://localhost:50051/healthz\n\n# Start control plane if needed\ncontrol-plane -addr :50051\n</code></pre></p>"},{"location":"cli/#invalid-node-id","title":"Invalid node ID","text":"<p>Error message: <code>failed to get node: node not found</code></p> <p>To resolve this issue, verify that the node ID exists: <pre><code>navarch list\n</code></pre></p>"},{"location":"cli/#control-plane-not-found","title":"Control plane not found","text":"<p>Error message: <code>failed to connect to control plane</code></p> <p>To resolve this issue, specify the correct control plane address using the <code>--server</code> flag:</p> <pre><code>navarch -s http://control-plane.example.com:50051 list\n</code></pre> <p>Or set the <code>NAVARCH_SERVER</code> environment variable:</p> <pre><code>export NAVARCH_SERVER=http://control-plane.example.com:50051\nnavarch list\n</code></pre>"},{"location":"cli/#whats-next","title":"What's next","text":"<ul> <li>For information about control plane and node agent architecture, see Architecture.</li> <li>For deployment instructions, see Deployment.</li> <li>To learn about extending Navarch with custom providers and health checks, see Extending Navarch.</li> </ul>"},{"location":"configuration/","title":"Configuration","text":"<p>Navarch uses a single YAML configuration file to define providers, pools, and server settings.</p>"},{"location":"configuration/#quick-start","title":"Quick start","text":"<pre><code>providers:\n  lambda:\n    type: lambda\n    api_key_env: LAMBDA_API_KEY\n\npools:\n  training:\n    provider: lambda\n    instance_type: gpu_8x_h100_sxm5\n    region: us-west-2\n    min_nodes: 2\n    max_nodes: 20\n    autoscaling:\n      type: reactive\n      scale_up_at: 80\n      scale_down_at: 20\n</code></pre> <p>Run the control plane with:</p> <pre><code>control-plane --config config.yaml\n</code></pre>"},{"location":"configuration/#server","title":"Server","text":"<pre><code>server:\n  address: \":50051\"              # Listen address\n  heartbeat_interval: 30s        # Node heartbeat frequency\n  health_check_interval: 60s     # Health check frequency\n  autoscale_interval: 30s        # Autoscaler evaluation frequency\n  health_policy: ./health-policy.yaml  # Custom health policy file\n  notifier:                   # Workload system integration\n    type: webhook\n    webhook:\n      cordon_url: https://scheduler.example.com/api/cordon\n      drain_url: https://scheduler.example.com/api/drain\n</code></pre> <p>All fields are optional with sensible defaults.</p> Field Default Description <code>address</code> <code>:50051</code> gRPC/HTTP listen address <code>heartbeat_interval</code> <code>30s</code> How often nodes send heartbeats <code>health_check_interval</code> <code>60s</code> How often health checks run <code>autoscale_interval</code> <code>30s</code> How often autoscaler evaluates <code>health_policy</code> (none) Path to health policy file <code>notifier</code> (none) Notifier configuration for workload system integration"},{"location":"configuration/#authentication","title":"Authentication","text":"<p>The control plane supports bearer token authentication:</p> <pre><code>export NAVARCH_AUTH_TOKEN=\"your-secret-token\"\ncontrol-plane --config config.yaml\n</code></pre> <p>See Authentication for client configuration and custom methods.</p>"},{"location":"configuration/#providers","title":"Providers","text":"<p>Providers define cloud platforms where GPU nodes are provisioned.</p> <pre><code>providers:\n  lambda:\n    type: lambda\n    api_key_env: LAMBDA_API_KEY    # Environment variable containing API key\n\n  gcp:\n    type: gcp\n    project: my-gcp-project\n\n  fake:\n    type: fake\n    gpu_count: 8                   # GPUs per fake instance (for testing)\n</code></pre> Type Description <code>lambda</code> Lambda Labs Cloud <code>gcp</code> Google Cloud Platform <code>aws</code> Amazon Web Services <code>fake</code> Fake provider for local development"},{"location":"configuration/#pools","title":"Pools","text":"<p>Pools define groups of GPU nodes with scaling policies.</p>"},{"location":"configuration/#single-provider-pool","title":"Single-provider pool","text":"<pre><code>pools:\n  training:\n    provider: lambda\n    instance_type: gpu_8x_h100_sxm5\n    region: us-west-2\n    min_nodes: 2\n    max_nodes: 20\n    cooldown: 5m\n    ssh_keys:\n      - ops-team\n    labels:\n      workload: training\n    autoscaling:\n      type: reactive\n      scale_up_at: 80\n      scale_down_at: 20\n    health:\n      unhealthy_after: 2\n      auto_replace: true\n</code></pre>"},{"location":"configuration/#multi-provider-pool","title":"Multi-provider pool","text":"<p>For fungible compute across multiple providers:</p> <pre><code>pools:\n  fungible:\n    providers:\n      - name: lambda\n        priority: 1\n        regions: [us-west-2, us-east-1]\n      - name: gcp\n        priority: 2\n        regions: [us-central1]\n        instance_type: a3-highgpu-8g    # Provider-specific override\n    strategy: priority\n    instance_type: h100-8x              # Abstract type\n    min_nodes: 4\n    max_nodes: 32\n</code></pre> <p>Provider selection strategies:</p> Strategy Description <code>priority</code> Try providers in priority order (lowest first) <code>cost</code> Select cheapest available provider <code>availability</code> Select first provider with capacity <code>round-robin</code> Distribute evenly across providers"},{"location":"configuration/#pool-fields","title":"Pool fields","text":"Field Required Description <code>provider</code> Yes* Single provider name <code>providers</code> Yes* List of provider entries (multi-provider) <code>strategy</code> No Provider selection strategy (multi-provider) <code>instance_type</code> Yes Instance type (provider-specific or abstract) <code>region</code> No Default region <code>zones</code> No Availability zones <code>ssh_keys</code> No SSH key names to install <code>min_nodes</code> Yes Minimum nodes to maintain <code>max_nodes</code> Yes Maximum nodes allowed <code>cooldown</code> No Time between scaling actions (default: 5m) <code>labels</code> No Key-value labels for workload routing <code>autoscaling</code> No Autoscaler configuration <code>health</code> No Health check configuration <code>setup_commands</code> No Bootstrap commands <code>ssh_user</code> No SSH username for bootstrap (default: <code>ubuntu</code>) <code>ssh_private_key_path</code> No Path to SSH private key for bootstrap <p>*Either <code>provider</code> or <code>providers</code> is required, but not both.</p>"},{"location":"configuration/#autoscaling","title":"Autoscaling","text":"<p>Configure how pools scale based on demand. See Autoscaling Concepts for details on each strategy.</p> <pre><code>autoscaling:\n  type: reactive          # reactive, queue, scheduled, predictive, composite\n  scale_up_at: 80         # Scale up when utilization &gt; 80%\n  scale_down_at: 20       # Scale down when utilization &lt; 20%\n</code></pre> Type Use case <code>reactive</code> Scale on current GPU utilization <code>queue</code> Scale on pending job count <code>scheduled</code> Time-based scaling limits <code>predictive</code> Forecast-based proactive scaling <code>composite</code> Combine multiple strategies"},{"location":"configuration/#health","title":"Health","text":"<p>Configure health checking and auto-replacement:</p> <pre><code>health:\n  unhealthy_after: 2     # Consecutive failures before unhealthy\n  auto_replace: true     # Automatically replace unhealthy nodes\n</code></pre> <p>See Health Monitoring for details on health events and XID errors.</p> <p>For custom health evaluation logic, see Health Policy.</p>"},{"location":"configuration/#notifier","title":"Notifier","text":"<p>The notifier integrates Navarch with external workload systems (job schedulers, Kubernetes, etc.). When nodes are cordoned or drained, the notifier notifies your workload system so it can stop scheduling new work and migrate existing workloads.</p>"},{"location":"configuration/#webhook-notifier","title":"Webhook notifier","text":"<p>Send HTTP notifications to your workload system:</p> <pre><code>server:\n  notifier:\n    type: webhook\n    webhook:\n      cordon_url: https://scheduler.example.com/api/v1/nodes/cordon\n      uncordon_url: https://scheduler.example.com/api/v1/nodes/uncordon\n      drain_url: https://scheduler.example.com/api/v1/nodes/drain\n      drain_status_url: https://scheduler.example.com/api/v1/nodes/drain-status\n      timeout: 30s\n      headers:\n        Authorization: Bearer ${SCHEDULER_TOKEN}\n</code></pre> Field Description <code>cordon_url</code> Called when a node is cordoned (POST) <code>uncordon_url</code> Called when a node is uncordoned (POST) <code>drain_url</code> Called when a node should be drained (POST) <code>drain_status_url</code> Polled to check if drain is complete (GET) <code>timeout</code> Request timeout (default: 30s) <code>headers</code> Custom headers for authentication"},{"location":"configuration/#webhook-payloads","title":"Webhook payloads","text":"<p>POST requests (cordon, uncordon, drain):</p> <pre><code>{\n  \"event\": \"cordon\",\n  \"node_id\": \"node-abc123\",\n  \"reason\": \"GPU failure detected\",\n  \"timestamp\": \"2024-01-15T10:30:00Z\"\n}\n</code></pre> <p>GET drain status request includes <code>?node_id=node-abc123</code> query parameter.</p> <p>Expected response:</p> <pre><code>{\n  \"drained\": true,\n  \"message\": \"All workloads evicted\"\n}\n</code></pre>"},{"location":"configuration/#no-notifier-default","title":"No notifier (default)","text":"<p>Without a notifier configured, cordon/drain/uncordon operations only update Navarch's internal state. Use this when:</p> <ul> <li>Running standalone without external schedulers</li> <li>Your workload system doesn't need notifications</li> <li>You're testing or developing locally</li> </ul> <pre><code>server:\n  notifier:\n    type: noop\n</code></pre>"},{"location":"configuration/#defaults","title":"Defaults","text":"<p>Apply defaults to all pools:</p> <pre><code>defaults:\n  ssh_keys:\n    - ops-team\n    - ml-team\n  ssh_user: ubuntu\n  ssh_private_key_path: ~/.ssh/navarch-key\n  health:\n    unhealthy_after: 2\n    auto_replace: true\n</code></pre>"},{"location":"configuration/#abstract-instance-types","title":"Abstract instance types","text":"<p>Use abstract types to provision equivalent hardware across providers:</p> Abstract Lambda GCP AWS <code>h100-8x</code> <code>gpu_8x_h100_sxm5</code> <code>a3-highgpu-8g</code> <code>p5.48xlarge</code> <code>h100-1x</code> <code>gpu_1x_h100_pcie</code> <code>a3-highgpu-1g</code> - <code>a100-8x</code> <code>gpu_8x_a100</code> <code>a2-highgpu-8g</code> <code>p4d.24xlarge</code> <code>a100-4x</code> <code>gpu_4x_a100</code> <code>a2-highgpu-4g</code> <code>p4de.24xlarge</code> <code>a100-1x</code> <code>gpu_1x_a100</code> <code>a2-highgpu-1g</code> -"},{"location":"configuration/#environment-variables","title":"Environment variables","text":"Variable Description <code>NAVARCH_AUTH_TOKEN</code> Authentication token for control plane <code>LAMBDA_API_KEY</code> Lambda Labs API key <code>GOOGLE_APPLICATION_CREDENTIALS</code> GCP credentials file path <code>AWS_ACCESS_KEY_ID</code> AWS access key <code>AWS_SECRET_ACCESS_KEY</code> AWS secret key"},{"location":"contributing/","title":"Contributing to Navarch","text":"<p>This guide covers how to contribute to Navarch, from setting up your development environment to submitting pull requests.</p>"},{"location":"contributing/#development-setup","title":"Development setup","text":""},{"location":"contributing/#prerequisites","title":"Prerequisites","text":"<ul> <li>Go 1.21 or later</li> <li>Make</li> <li>Docker (optional, for Docker provider tests)</li> <li>GPU machine (optional, for hardware testing)</li> </ul>"},{"location":"contributing/#clone-and-build","title":"Clone and build","text":"<pre><code>git clone https://github.com/NavarchProject/navarch.git\ncd navarch\nmake build\n</code></pre> <p>This produces binaries in <code>bin/</code>: - <code>control-plane</code> - the control plane server - <code>node</code> - the node agent - <code>navarch</code> - the CLI - <code>simulator</code> - the fleet simulator</p>"},{"location":"contributing/#run-tests","title":"Run tests","text":"<pre><code>make test          # Run all tests\nmake test-race     # With race detection\nmake test-all      # Format, lint, and test\n</code></pre> <p>See Testing for the Docker provider and simulator.</p>"},{"location":"contributing/#code-structure","title":"Code structure","text":"<pre><code>navarch/\n\u251c\u2500\u2500 cmd/                    # Entry points\n\u2502   \u251c\u2500\u2500 control-plane/      # Control plane server\n\u2502   \u251c\u2500\u2500 node/               # Node agent\n\u2502   \u251c\u2500\u2500 navarch/            # CLI\n\u2502   \u2514\u2500\u2500 simulator/          # Fleet simulator\n\u251c\u2500\u2500 pkg/\n\u2502   \u251c\u2500\u2500 bootstrap/          # SSH bootstrap for setup commands\n\u2502   \u251c\u2500\u2500 config/             # Configuration parsing\n\u2502   \u251c\u2500\u2500 controlplane/       # Control plane logic\n\u2502   \u251c\u2500\u2500 health/             # Health policy evaluation\n\u2502   \u251c\u2500\u2500 node/               # Node agent logic\n\u2502   \u251c\u2500\u2500 pool/               # Pool management and autoscaling\n\u2502   \u2514\u2500\u2500 provider/           # Cloud provider interfaces\n\u2502       \u251c\u2500\u2500 docker/         # Docker provider (testing)\n\u2502       \u251c\u2500\u2500 fake/           # Fake provider (testing)\n\u2502       \u251c\u2500\u2500 gcp/            # Google Cloud\n\u2502       \u2514\u2500\u2500 lambda/         # Lambda Labs\n\u251c\u2500\u2500 proto/                  # Protocol buffer definitions\n\u2514\u2500\u2500 scenarios/              # Simulator test scenarios\n</code></pre>"},{"location":"contributing/#making-changes","title":"Making changes","text":""},{"location":"contributing/#branch-naming","title":"Branch naming","text":"<p>Use descriptive branch names: - <code>feature/add-azure-provider</code> - <code>fix/health-check-timeout</code> - <code>docs/improve-deployment-guide</code></p>"},{"location":"contributing/#commit-messages","title":"Commit messages","text":"<p>Write clear commit messages that explain what and why:</p> <pre><code>Add cooldown period to autoscaler\n\nThe autoscaler was scaling up and down too frequently when utilization\nhovered near the threshold. This adds a configurable cooldown period\nthat prevents scaling actions within a specified duration of each other.\n\nFixes #123\n</code></pre>"},{"location":"contributing/#testing-requirements","title":"Testing requirements","text":"<p>All changes must include tests: - Unit tests for new functions - Integration tests for new features - Simulator scenarios for behavior changes</p> <p>Run the full test suite before submitting:</p> <pre><code>make test-all\n</code></pre>"},{"location":"contributing/#pull-request-process","title":"Pull request process","text":"<ol> <li>Fork and branch: Create a feature branch from <code>main</code></li> <li>Make changes: Write code and tests</li> <li>Test locally: Run <code>make test-all</code></li> <li>Push and open PR: Include a clear description of the change</li> <li>Address feedback: Respond to code review comments</li> <li>Merge: A maintainer will merge once approved</li> </ol>"},{"location":"contributing/#pr-description-template","title":"PR description template","text":"<pre><code>## What\n\nBrief description of the change.\n\n## Why\n\nWhy is this change needed? Link to issue if applicable.\n\n## How\n\nHow does this change work? Any design decisions worth noting?\n\n## Testing\n\nHow was this tested? Include simulator scenarios if applicable.\n</code></pre>"},{"location":"contributing/#adding-a-new-cloud-provider","title":"Adding a new cloud provider","text":"<p>See Extending Navarch for the provider interface and implementation guide.</p>"},{"location":"contributing/#adding-a-new-autoscaler","title":"Adding a new autoscaler","text":"<p>See Extending Navarch for the autoscaler interface and implementation guide.</p>"},{"location":"contributing/#documentation","title":"Documentation","text":"<p>Documentation lives in the <code>website/</code> directory and uses MkDocs Material.</p> <p>To preview documentation changes:</p> <pre><code>cd website\npip install mkdocs-material\nmkdocs serve\n</code></pre>"},{"location":"contributing/#getting-help","title":"Getting help","text":"<ul> <li>Open an issue for bugs or feature requests</li> <li>Start a discussion for questions or ideas</li> </ul>"},{"location":"contributing/#code-of-conduct","title":"Code of conduct","text":"<p>Be respectful and constructive. We're all here to build something useful.</p>"},{"location":"deployment/","title":"Deployment architecture","text":"<p>This document describes how to deploy Navarch in production, including agent installation, custom images, and autoscaling.</p>"},{"location":"deployment/#architecture-overview","title":"Architecture overview","text":"<p>The control plane is the single source of truth. It provisions instances through provider adapters and receives health reports from node agents.</p> <pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                         Control Plane                                \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u2502\n\u2502  \u2502 API Server  \u2502  \u2502 Pool        \u2502  \u2502 Health      \u2502  \u2502 Provider  \u2502  \u2502\n\u2502  \u2502             \u2502  \u2502 Manager     \u2502  \u2502 Monitor     \u2502  \u2502 Registry  \u2502  \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2518  \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                           \u2502                                \u2502\n              Provision/   \u2502                                \u2502 Routes to\n              Terminate    \u2502                                \u2502 provider\n                           \u25bc                                \u25bc\n                  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n                  \u2502              Provider Adapters                   \u2502\n                  \u2502  [GCP]  [AWS]  [Lambda]  [CoreWeave]  [Custom]  \u2502\n                  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                                       \u2502\n                                       \u2502 Cloud APIs\n                                       \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                         GPU Node Pool                                \u2502\n\u2502                                                                      \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510              \u2502\n\u2502  \u2502  Node 1      \u2502  \u2502  Node 2      \u2502  \u2502  Node N      \u2502              \u2502\n\u2502  \u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u2502  \u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u2502  \u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u2502              \u2502\n\u2502  \u2502  \u2502 Agent  \u2502\u2500\u2500\u253c\u2500\u2500\u253c\u2500\u2500\u2502 Agent  \u2502\u2500\u2500\u253c\u2500\u2500\u253c\u2500\u2500\u2502 Agent  \u2502\u2500\u2500\u253c\u2500\u2500\u2500 Health    \u2502\n\u2502  \u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2502  \u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2502  \u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2502    reports   \u2502\n\u2502  \u2502  8x H100     \u2502  \u2502  8x H100     \u2502  \u2502  8x H100     \u2502    to CP     \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518              \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>Key principle: Nodes do not manage their own lifecycle. The control plane decides when to provision and terminate instances. Node agents only report health.</p>"},{"location":"deployment/#node-agent-deployment","title":"Node agent deployment","text":"<p>There are several ways to deploy the node agent. Choose the approach that best fits your infrastructure:</p> Method Best for Pros Cons Custom images Production Fast startup, consistent Requires image pipeline Cloud-init Getting started Simple, no image build Slower startup SSH bootstrap Managed fleets Control plane manages setup Requires SSH access Container Kubernetes Cloud-native Additional abstraction"},{"location":"deployment/#option-1-custom-machine-images-recommended","title":"Option 1: Custom machine images (recommended)","text":"<p>Create custom images with the Navarch agent pre-installed.</p> <p>Benefits:</p> <ul> <li>Agent starts automatically on boot.</li> <li>Consistent configuration across all nodes.</li> <li>Faster instance startup (no runtime installation).</li> <li>Version control for agent updates.</li> </ul> <p>GCP custom image:</p> <pre><code># 1. Create a base instance\ngcloud compute instances create navarch-base \\\n  --zone=us-central1-a \\\n  --machine-type=n1-standard-4 \\\n  --image-family=ubuntu-2204-lts \\\n  --image-project=ubuntu-os-cloud\n\n# 2. SSH and install agent\ngcloud compute ssh navarch-base --zone=us-central1-a\n\n# Install NVIDIA driver, Go, Navarch agent\nsudo apt-get update\nsudo apt-get install -y nvidia-driver-535\n# ... install navarch-node binary and systemd service\n\n# 3. Create image\ngcloud compute instances stop navarch-base --zone=us-central1-a\ngcloud compute images create navarch-gpu-v1 \\\n  --source-disk=navarch-base \\\n  --source-disk-zone=us-central1-a \\\n  --family=navarch-gpu\n</code></pre> <p>AWS AMI:</p> <pre><code># Use Packer or AWS Image Builder\n# See examples/packer/ for Packer templates\n</code></pre>"},{"location":"deployment/#option-2-cloud-init-user-data","title":"Option 2: Cloud-init / user-data","text":"<p>Install agent at instance startup using cloud-init.</p> <p>GCP startup script:</p> <pre><code>gcloud compute instances create gpu-node-1 \\\n  --metadata-from-file startup-script=scripts/install-agent.sh\n</code></pre> <p>install-agent.sh:</p> <pre><code>#!/bin/bash\nset -e\n\n# Download and install agent\ncurl -L https://github.com/NavarchProject/navarch/releases/latest/download/navarch-node-linux-amd64 \\\n  -o /usr/local/bin/navarch-node\nchmod +x /usr/local/bin/navarch-node\n\n# Get instance metadata\nINSTANCE_ID=$(curl -s http://metadata.google.internal/computeMetadata/v1/instance/id -H \"Metadata-Flavor: Google\")\nZONE=$(curl -s http://metadata.google.internal/computeMetadata/v1/instance/zone -H \"Metadata-Flavor: Google\" | cut -d/ -f4)\nREGION=$(echo $ZONE | rev | cut -d- -f2- | rev)\n\n# Create systemd service\ncat &gt; /etc/systemd/system/navarch-node.service &lt;&lt; EOF\n[Unit]\nDescription=Navarch Node Agent\nAfter=network.target nvidia-persistenced.service\n\n[Service]\nType=simple\nExecStart=/usr/local/bin/navarch-node \\\n  --server https://control-plane.example.com \\\n  --node-id ${INSTANCE_ID} \\\n  --provider gcp \\\n  --region ${REGION} \\\n  --zone ${ZONE}\nRestart=always\nRestartSec=10\n\n[Install]\nWantedBy=multi-user.target\nEOF\n\nsystemctl daemon-reload\nsystemctl enable navarch-node\nsystemctl start navarch-node\n</code></pre>"},{"location":"deployment/#option-3-container-deployment","title":"Option 3: Container deployment","text":"<p>Run the agent as a container (useful for Kubernetes).</p> <p>Docker:</p> <pre><code>docker run -d \\\n  --name navarch-node \\\n  --privileged \\\n  --gpus all \\\n  -v /var/log:/var/log:ro \\\n  navarch/node:latest \\\n  --server https://control-plane.example.com \\\n  --node-id $(hostname)\n</code></pre> <p>Kubernetes DaemonSet:</p> <pre><code>apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: navarch-node\nspec:\n  selector:\n    matchLabels:\n      app: navarch-node\n  template:\n    spec:\n      containers:\n      - name: navarch-node\n        image: navarch/node:latest\n        args:\n        - --server=https://control-plane.example.com\n        securityContext:\n          privileged: true\n        volumeMounts:\n        - name: dev\n          mountPath: /dev\n      volumes:\n      - name: dev\n        hostPath:\n          path: /dev\n      nodeSelector:\n        nvidia.com/gpu: \"true\"\n</code></pre>"},{"location":"deployment/#option-4-ssh-bootstrap","title":"Option 4: SSH bootstrap","text":"<p>Let the control plane automatically install and configure the agent via SSH when nodes are provisioned. This is useful when you want centralized control over node setup without building custom images.</p> <p>Configuration:</p> <pre><code>pools:\n  training:\n    provider: lambda\n    instance_type: gpu_8x_h100_sxm5\n    min_nodes: 2\n    max_nodes: 20\n    ssh_user: ubuntu\n    ssh_private_key_path: ~/.ssh/navarch-key\n    setup_commands:\n      - |\n        curl -L https://github.com/NavarchProject/navarch/releases/latest/download/navarch-node-linux-amd64 \\\n          -o /usr/local/bin/navarch-node &amp;&amp; chmod +x /usr/local/bin/navarch-node\n      - |\n        cat &gt; /etc/systemd/system/navarch-node.service &lt;&lt; EOF\n        [Unit]\n        Description=Navarch Node Agent\n        After=network.target\n\n        [Service]\n        ExecStart=/usr/local/bin/navarch-node --server {{.ControlPlane}} --node-id {{.NodeID}}\n        Restart=always\n\n        [Install]\n        WantedBy=multi-user.target\n        EOF\n      - systemctl daemon-reload &amp;&amp; systemctl enable navarch-node &amp;&amp; systemctl start navarch-node\n</code></pre> <p>The control plane:</p> <ol> <li>Provisions the instance through the provider API</li> <li>Waits for SSH to become available (up to 10 minutes)</li> <li>Connects and runs each setup command in order</li> <li>Logs detailed timing and output for each phase</li> </ol> <p>Template variables like <code>{{.ControlPlane}}</code>, <code>{{.NodeID}}</code>, <code>{{.Pool}}</code>, <code>{{.Provider}}</code>, <code>{{.Region}}</code>, and <code>{{.InstanceType}}</code> are available in setup commands.</p> <p>For full details, see Node Bootstrap.</p>"},{"location":"deployment/#systemd-service-configuration","title":"Systemd service configuration","text":"<p>For production deployments, run the agent as a systemd service.</p> <p>/etc/systemd/system/navarch-node.service:</p> <pre><code>[Unit]\nDescription=Navarch Node Agent\nDocumentation=https://github.com/NavarchProject/navarch\nAfter=network-online.target nvidia-persistenced.service\nWants=network-online.target\n\n[Service]\nType=simple\nUser=root\nExecStart=/usr/local/bin/navarch-node \\\n  --server https://control-plane.example.com \\\n  --node-id %H\nRestart=always\nRestartSec=10\nStandardOutput=journal\nStandardError=journal\n\n# Security hardening\nNoNewPrivileges=no\nProtectSystem=strict\nReadWritePaths=/var/log\n\n[Install]\nWantedBy=multi-user.target\n</code></pre> <p>Managing the service:</p> <pre><code># Enable on boot\nsudo systemctl enable navarch-node\n\n# Start/stop/restart\nsudo systemctl start navarch-node\nsudo systemctl stop navarch-node\nsudo systemctl restart navarch-node\n\n# View logs\njournalctl -u navarch-node -f\n</code></pre>"},{"location":"deployment/#pool-management","title":"Pool management","text":"<p>The control plane manages GPU pools directly through provider adapters. There is no separate autoscaler component.</p>"},{"location":"deployment/#architecture","title":"Architecture","text":"<pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                         Control Plane                                \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510                 \u2502\n\u2502  \u2502 Pool        \u2502  \u2502 Provider    \u2502  \u2502 Health      \u2502                 \u2502\n\u2502  \u2502 Manager     \u2502\u2500\u2500\u2502 Registry    \u2502  \u2502 Monitor     \u2502                 \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2518                 \u2502\n\u2502         \u2502                \u2502                \u2502                         \u2502\n\u2502         \u2502 Decides        \u2502 Routes to      \u2502 Reports                 \u2502\n\u2502         \u2502 scale actions  \u2502 correct adapter\u2502 unhealthy nodes        \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n          \u2502                \u2502                \u2502\n          \u25bc                \u25bc                \u25bc\n   \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n   \u2502                    Provider Adapters                         \u2502\n   \u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510        \u2502\n   \u2502  \u2502  GCP    \u2502  \u2502  AWS    \u2502  \u2502 Lambda  \u2502  \u2502 Custom  \u2502        \u2502\n   \u2502  \u2502 Adapter \u2502  \u2502 Adapter \u2502  \u2502 Adapter \u2502  \u2502 Adapter \u2502        \u2502\n   \u2502  \u2514\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2518        \u2502\n   \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n           \u2502            \u2502            \u2502            \u2502\n           \u25bc            \u25bc            \u25bc            \u25bc\n      [GCP API]    [AWS API]   [Lambda API]   [Your API]\n</code></pre>"},{"location":"deployment/#provider-interface","title":"Provider interface","text":"<p>Each cloud provider implements a simple interface:</p> <pre><code>type Provider interface {\n    Name() string\n    Provision(ctx context.Context, req ProvisionRequest) (*Node, error)\n    Terminate(ctx context.Context, nodeID string) error\n    List(ctx context.Context) ([]*Node, error)\n}\n</code></pre> <p>The control plane calls these methods to manage instances. The node agent just reports health; it does not manage its own lifecycle.</p>"},{"location":"deployment/#pool-manager-responsibilities","title":"Pool manager responsibilities","text":"<p>The pool manager (part of control plane) handles:</p> <ol> <li>Scaling decisions: When to add or remove nodes.</li> <li>Health-based replacement: Terminate unhealthy, provision replacement.</li> <li>Pool constraints: Min/max nodes, instance types, zones.</li> <li>Provider selection: Multi-cloud routing based on availability and cost.</li> </ol>"},{"location":"deployment/#scaling-triggers","title":"Scaling triggers","text":"<p>Scale up when: - Pool size below minimum. - All healthy nodes at capacity. - Pending provision requests.</p> <p>Scale down when: - Pool size above maximum. - Low utilization for extended period. - Node reported unhealthy (terminate and replace).</p>"},{"location":"deployment/#health-based-replacement","title":"Health-based replacement","text":"<p>When a node becomes unhealthy, the control plane handles it:</p> <pre><code>Health Monitor detects unhealthy node\n                \u2502\n                \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 Is failure fatal?             \u2502\n\u2502 (XID 79, 48, 94, 95, etc.)   \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                \u2502\n       Yes      \u2502      No\n        \u25bc       \u2502       \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 1. Cordon     \u2502  \u2502 Log warning     \u2502\n\u2502 2. Drain      \u2502  \u2502 Continue        \u2502\n\u2502 3. Terminate  \u2502  \u2502 monitoring      \u2502\n\u2502 4. Provision  \u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\u2502    replacement\u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>The control plane orchestrates the entire flow:</p> <pre><code>// Pseudocode for health-based replacement\nfunc (pm *PoolManager) handleUnhealthyNode(ctx context.Context, node *Node) error {\n    // Cordon: prevent new workloads\n    if err := pm.cordon(ctx, node.ID); err != nil {\n        return err\n    }\n\n    // Drain: wait for workloads to complete or migrate\n    if err := pm.drain(ctx, node.ID); err != nil {\n        return err\n    }\n\n    // Terminate through provider adapter\n    provider := pm.registry.Get(node.Provider)\n    if err := provider.Terminate(ctx, node.ID); err != nil {\n        return err\n    }\n\n    // Provision replacement\n    _, err := provider.Provision(ctx, ProvisionRequest{\n        Type:     node.InstanceType,\n        GPUCount: node.GPUCount,\n    })\n    return err\n}\n</code></pre>"},{"location":"deployment/#pool-configuration","title":"Pool configuration","text":"<pre><code>providers:\n  gcp:\n    type: gcp\n    project: my-gcp-project\n\n  aws:\n    type: aws\n    region: us-east-1\n\npools:\n  training:\n    provider: gcp\n    instance_type: a3-highgpu-8g\n    region: us-central1\n    zones: [us-central1-a, us-central1-b]\n    min_nodes: 2\n    max_nodes: 100\n    cooldown: 10m\n    autoscaling:\n      type: reactive\n      scale_up_at: 80\n      scale_down_at: 20\n    health:\n      unhealthy_after: 2\n      auto_replace: true\n\n  inference:\n    provider: aws\n    instance_type: p4d.24xlarge\n    region: us-east-1\n    min_nodes: 4\n    max_nodes: 50\n</code></pre>"},{"location":"deployment/#multi-cloud-support","title":"Multi-cloud support","text":"<p>The control plane can manage pools across multiple providers:</p> <pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                      Control Plane                            \u2502\n\u2502                                                               \u2502\n\u2502  Pool: training-pool (GCP)    Pool: inference-pool (AWS)    \u2502\n\u2502  \u251c\u2500\u2500 node-gcp-1              \u251c\u2500\u2500 node-aws-1                 \u2502\n\u2502  \u251c\u2500\u2500 node-gcp-2              \u251c\u2500\u2500 node-aws-2                 \u2502\n\u2502  \u2514\u2500\u2500 node-gcp-3              \u2514\u2500\u2500 node-aws-3                 \u2502\n\u2502                                                               \u2502\n\u2502  Pool: burst-pool (Lambda)                                   \u2502\n\u2502  \u2514\u2500\u2500 (scales 0 to N on demand)                              \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"deployment/#provider-adapter-implementation","title":"Provider adapter implementation","text":"<p>To add a new cloud provider, implement the interface:</p> <pre><code>package lambda\n\ntype LambdaProvider struct {\n    apiKey string\n    client *lambda.Client\n}\n\nfunc (p *LambdaProvider) Name() string {\n    return \"lambda\"\n}\n\nfunc (p *LambdaProvider) Provision(ctx context.Context, req provider.ProvisionRequest) (*provider.Node, error) {\n    // Call Lambda Labs API to create instance\n    instance, err := p.client.LaunchInstance(ctx, &amp;lambda.LaunchRequest{\n        InstanceType: req.Type,\n        Name:         req.Name,\n    })\n    if err != nil {\n        return nil, err\n    }\n\n    return &amp;provider.Node{\n        ID:       instance.ID,\n        Provider: \"lambda\",\n        Type:     instance.Type,\n        Status:   \"provisioning\",\n    }, nil\n}\n\nfunc (p *LambdaProvider) Terminate(ctx context.Context, nodeID string) error {\n    return p.client.TerminateInstance(ctx, nodeID)\n}\n\nfunc (p *LambdaProvider) List(ctx context.Context) ([]*provider.Node, error) {\n    instances, err := p.client.ListInstances(ctx)\n    // ... convert to provider.Node\n}\n</code></pre>"},{"location":"deployment/#high-availability","title":"High availability","text":""},{"location":"deployment/#control-plane-ha","title":"Control plane HA","text":"<p>For production, run multiple control plane replicas:</p> <pre><code>                    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n                    \u2502  Load Balancer  \u2502\n                    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                             \u2502\n         \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n         \u25bc                   \u25bc                   \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 Control Plane 1 \u2502 \u2502 Control Plane 2 \u2502 \u2502 Control Plane 3 \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n         \u2502                   \u2502                   \u2502\n         \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                             \u2502\n                    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n                    \u2502    Database     \u2502\n                    \u2502   (Postgres)    \u2502\n                    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"deployment/#node-agent-resilience","title":"Node agent resilience","text":"<p>The node agent is designed to be resilient:</p> <ul> <li>Automatic reconnection on control plane failure.</li> <li>Local health check caching during disconnection.</li> <li>Graceful degradation when control plane is unavailable.</li> </ul>"},{"location":"deployment/#monitoring-and-observability","title":"Monitoring and observability","text":""},{"location":"deployment/#metrics-to-export","title":"Metrics to export","text":"<pre><code># Node metrics\nnavarch_node_gpu_count\nnavarch_node_gpu_temperature_celsius\nnavarch_node_gpu_utilization_percent\nnavarch_node_gpu_memory_used_bytes\nnavarch_node_gpu_power_watts\nnavarch_node_health_check_status\n\n# Control plane metrics\nnavarch_nodes_total\nnavarch_nodes_healthy\nnavarch_nodes_unhealthy\nnavarch_commands_issued_total\nnavarch_xid_errors_total\n</code></pre>"},{"location":"deployment/#alerting-recommendations","title":"Alerting recommendations","text":"Alert Condition Severity GPU temperature high &gt; 83\u00b0C for 5 min Warning GPU fallen off bus XID 79 Critical Node unhealthy Health != Healthy for 10 min Warning No heartbeat Last heartbeat &gt; 5 min Critical Pool capacity low Available nodes &lt; 10% Warning"},{"location":"deployment/#security-considerations","title":"Security considerations","text":""},{"location":"deployment/#network-security","title":"Network security","text":"<ul> <li>Control plane should be behind a load balancer with TLS.</li> <li>Node agents should authenticate with the control plane.</li> <li>Consider using private networks for node-to-control-plane traffic.</li> </ul>"},{"location":"deployment/#authentication","title":"Authentication","text":"<p>Enable bearer token authentication by setting <code>NAVARCH_AUTH_TOKEN</code> on both the control plane and node agents:</p> <pre><code># Control plane\nexport NAVARCH_AUTH_TOKEN=\"your-secret-token\"\ncontrol-plane --config config.yaml\n\n# Node agent\nexport NAVARCH_AUTH_TOKEN=\"your-secret-token\"\nnode-agent --server https://control-plane.example.com\n</code></pre> <p>For token generation, client configuration, and custom authentication methods, see authentication.</p>"},{"location":"deployment/#secrets-management","title":"Secrets management","text":"<ul> <li>Use cloud provider secret managers for sensitive configuration.</li> <li>Rotate credentials regularly.</li> <li>Avoid hardcoding secrets in images or scripts.</li> </ul>"},{"location":"extending/","title":"Extending Navarch","text":"<p>Navarch is designed to be extended. You can add custom cloud providers, autoscaling strategies, workload notifiers, and metrics sources without modifying the core codebase.</p>"},{"location":"extending/#custom-providers","title":"Custom providers","text":"<p>Providers handle instance lifecycle: provisioning, listing, and terminating GPU instances.</p>"},{"location":"extending/#provider-interface","title":"Provider interface","text":"<pre><code>type Provider interface {\n    // Name returns the provider identifier (e.g., \"lambda\", \"gcp\", \"aws\").\n    Name() string\n\n    // Provision creates a new instance with the given specification.\n    Provision(ctx context.Context, req ProvisionRequest) (*Instance, error)\n\n    // Terminate destroys an instance.\n    Terminate(ctx context.Context, instanceID string) error\n\n    // List returns all instances managed by this provider.\n    List(ctx context.Context) ([]*Instance, error)\n}\n</code></pre>"},{"location":"extending/#provisionrequest","title":"ProvisionRequest","text":"<pre><code>type ProvisionRequest struct {\n    InstanceType string            // Provider-specific instance type\n    Region       string            // Region to provision in\n    Zone         string            // Optional: specific zone\n    SSHKeyNames  []string          // SSH keys to inject\n    Labels       map[string]string // Labels to apply\n    UserData     string            // Cloud-init or startup script\n}\n</code></pre>"},{"location":"extending/#example-custom-provider","title":"Example: Custom provider","text":"<pre><code>package myprovider\n\nimport (\n    \"context\"\n    \"github.com/NavarchProject/navarch/pkg/provider\"\n)\n\ntype MyCloudProvider struct {\n    client *MyCloudClient\n    region string\n}\n\nfunc New(apiKey, region string) (*MyCloudProvider, error) {\n    client, err := NewMyCloudClient(apiKey)\n    if err != nil {\n        return nil, err\n    }\n    return &amp;MyCloudProvider{client: client, region: region}, nil\n}\n\nfunc (p *MyCloudProvider) Name() string {\n    return \"mycloud\"\n}\n\nfunc (p *MyCloudProvider) Provision(ctx context.Context, req provider.ProvisionRequest) (*provider.Instance, error) {\n    resp, err := p.client.CreateInstance(ctx, CreateRequest{\n        Type:     req.InstanceType,\n        Region:   p.region,\n        SSHKeys:  req.SSHKeyNames,\n        UserData: req.UserData,\n    })\n    if err != nil {\n        return nil, fmt.Errorf(\"create instance: %w\", err)\n    }\n\n    return &amp;provider.Instance{\n        ID:           resp.ID,\n        ProviderName: p.Name(),\n        Status:       provider.StatusPending,\n        Region:       p.region,\n        InstanceType: req.InstanceType,\n    }, nil\n}\n\nfunc (p *MyCloudProvider) Terminate(ctx context.Context, instanceID string) error {\n    return p.client.DeleteInstance(ctx, instanceID)\n}\n\nfunc (p *MyCloudProvider) List(ctx context.Context) ([]*provider.Instance, error) {\n    instances, err := p.client.ListInstances(ctx)\n    if err != nil {\n        return nil, err\n    }\n\n    result := make([]*provider.Instance, len(instances))\n    for i, inst := range instances {\n        result[i] = &amp;provider.Instance{\n            ID:           inst.ID,\n            ProviderName: p.Name(),\n            Status:       mapStatus(inst.State),\n            PublicIP:     inst.PublicIP,\n            PrivateIP:    inst.PrivateIP,\n            Region:       inst.Region,\n        }\n    }\n    return result, nil\n}\n</code></pre> <p>See <code>pkg/provider/lambda/</code> for a production implementation. For testing, see the Docker provider.</p>"},{"location":"extending/#registering-your-provider","title":"Registering your provider","text":"<pre><code>myCloud, err := myprovider.New(os.Getenv(\"MYCLOUD_API_KEY\"), \"us-east-1\")\nif err != nil {\n    log.Fatal(err)\n}\n\ncontrolPlane.RegisterProvider(myCloud)\n</code></pre>"},{"location":"extending/#custom-autoscalers","title":"Custom autoscalers","text":"<p>Autoscalers decide when to scale pools up or down based on metrics.</p>"},{"location":"extending/#autoscaler-interface","title":"Autoscaler interface","text":"<pre><code>type Autoscaler interface {\n    // Name returns the autoscaler type identifier.\n    Name() string\n\n    // Recommend returns the target node count for the pool.\n    Recommend(ctx context.Context, state PoolState) (ScaleRecommendation, error)\n}\n\ntype PoolState struct {\n    CurrentNodes int\n    MinNodes     int\n    MaxNodes     int\n    Metrics      PoolMetrics\n}\n\ntype PoolMetrics struct {\n    Utilization        float64   // Average GPU utilization (0-100)\n    PendingJobs        int       // Jobs waiting to be scheduled\n    QueueDepth         int       // Total jobs in queue\n    UtilizationHistory []float64 // Historical samples for trend analysis\n}\n\ntype ScaleRecommendation struct {\n    TargetNodes int\n    Reason      string\n}\n</code></pre>"},{"location":"extending/#example-custom-autoscaler","title":"Example: Custom autoscaler","text":"<p>A cost-aware autoscaler that scales down aggressively outside business hours:</p> <pre><code>package costscaler\n\nimport (\n    \"context\"\n    \"time\"\n    \"github.com/NavarchProject/navarch/pkg/pool\"\n)\n\ntype CostAwareScaler struct {\n    peakStart     int     // Hour (0-23)\n    peakEnd       int\n    peakThreshold float64 // Scale up when utilization exceeds this\n    offPeakMin    int     // Minimum nodes outside peak hours\n}\n\nfunc New(peakStart, peakEnd int, threshold float64, offPeakMin int) *CostAwareScaler {\n    return &amp;CostAwareScaler{\n        peakStart:     peakStart,\n        peakEnd:       peakEnd,\n        peakThreshold: threshold,\n        offPeakMin:    offPeakMin,\n    }\n}\n\nfunc (s *CostAwareScaler) Name() string {\n    return \"cost-aware\"\n}\n\nfunc (s *CostAwareScaler) Recommend(ctx context.Context, state pool.PoolState) (pool.ScaleRecommendation, error) {\n    hour := time.Now().Hour()\n    isPeak := hour &gt;= s.peakStart &amp;&amp; hour &lt; s.peakEnd\n\n    current := state.CurrentNodes\n    util := state.Metrics.Utilization\n\n    // Outside peak hours, scale to minimum\n    if !isPeak {\n        target := s.offPeakMin\n        if target &lt; state.MinNodes {\n            target = state.MinNodes\n        }\n        return pool.ScaleRecommendation{\n            TargetNodes: target,\n            Reason:      \"off-peak hours, scaling to minimum\",\n        }, nil\n    }\n\n    // During peak, scale based on utilization\n    if util &gt; s.peakThreshold &amp;&amp; current &lt; state.MaxNodes {\n        return pool.ScaleRecommendation{\n            TargetNodes: current + 1,\n            Reason:      fmt.Sprintf(\"utilization %.1f%% exceeds threshold\", util),\n        }, nil\n    }\n\n    if util &lt; s.peakThreshold/2 &amp;&amp; current &gt; state.MinNodes {\n        return pool.ScaleRecommendation{\n            TargetNodes: current - 1,\n            Reason:      fmt.Sprintf(\"utilization %.1f%% below threshold\", util),\n        }, nil\n    }\n\n    return pool.ScaleRecommendation{\n        TargetNodes: current,\n        Reason:      \"no change needed\",\n    }, nil\n}\n</code></pre> <p>See <code>pkg/pool/autoscaler.go</code> for built-in implementations (reactive, queue, scheduled, predictive, composite).</p>"},{"location":"extending/#custom-metrics-sources","title":"Custom metrics sources","text":"<p>For queue-based or custom autoscaling, provide metrics from your scheduler.</p>"},{"location":"extending/#metricssource-interface","title":"MetricsSource interface","text":"<pre><code>type MetricsSource interface {\n    GetPoolMetrics(ctx context.Context, poolName string) (*PoolMetrics, error)\n}\n</code></pre>"},{"location":"extending/#example-kubernetes-integration","title":"Example: Kubernetes integration","text":"<pre><code>type K8sMetricsSource struct {\n    client    *kubernetes.Clientset\n    namespace string\n}\n\nfunc (m *K8sMetricsSource) GetPoolMetrics(ctx context.Context, poolName string) (*PoolMetrics, error) {\n    pods, err := m.client.CoreV1().Pods(m.namespace).List(ctx, metav1.ListOptions{\n        LabelSelector: \"navarch.dev/pool=\" + poolName,\n    })\n    if err != nil {\n        return nil, err\n    }\n\n    var pending, running int\n    for _, pod := range pods.Items {\n        switch pod.Status.Phase {\n        case corev1.PodPending:\n            pending++\n        case corev1.PodRunning:\n            running++\n        }\n    }\n\n    return &amp;PoolMetrics{\n        PendingJobs: pending,\n        QueueDepth:  pending + running,\n    }, nil\n}\n</code></pre>"},{"location":"extending/#example-slurm-integration","title":"Example: Slurm integration","text":"<pre><code>type SlurmMetricsSource struct {\n    partition string\n}\n\nfunc (m *SlurmMetricsSource) GetPoolMetrics(ctx context.Context, poolName string) (*PoolMetrics, error) {\n    // Query pending jobs\n    out, err := exec.CommandContext(ctx, \"squeue\", \"-p\", m.partition, \"-t\", \"PENDING\", \"-h\", \"-o\", \"%i\").Output()\n    if err != nil {\n        return nil, err\n    }\n    pending := len(strings.Split(strings.TrimSpace(string(out)), \"\\n\"))\n\n    // Query running jobs\n    out, err = exec.CommandContext(ctx, \"squeue\", \"-p\", m.partition, \"-t\", \"RUNNING\", \"-h\", \"-o\", \"%i\").Output()\n    if err != nil {\n        return nil, err\n    }\n    running := len(strings.Split(strings.TrimSpace(string(out)), \"\\n\"))\n\n    return &amp;PoolMetrics{\n        PendingJobs: pending,\n        QueueDepth:  pending + running,\n    }, nil\n}\n</code></pre>"},{"location":"extending/#custom-notifiers","title":"Custom notifiers","text":"<p>Notifiers integrate Navarch with your workload management system. When Navarch cordons, drains, or uncordons a node, the notifier tells your scheduler (Kubernetes, Slurm, Ray, or custom) to stop scheduling work and migrate existing workloads.</p>"},{"location":"extending/#notifier-interface","title":"Notifier interface","text":"<pre><code>type Notifier interface {\n    // Cordon marks a node as unschedulable. The workload system should stop\n    // placing new workloads on this node. Existing workloads continue.\n    Cordon(ctx context.Context, nodeID string, reason string) error\n\n    // Uncordon marks a node as schedulable again.\n    Uncordon(ctx context.Context, nodeID string) error\n\n    // Drain requests the workload system to migrate workloads off the node.\n    Drain(ctx context.Context, nodeID string, reason string) error\n\n    // IsDrained returns true if all workloads have been migrated off\n    // the node and it's safe to terminate.\n    IsDrained(ctx context.Context, nodeID string) (bool, error)\n\n    // Name returns the notifier name for logging.\n    Name() string\n}\n</code></pre>"},{"location":"extending/#example-kubernetes-notifier","title":"Example: Kubernetes notifier","text":"<pre><code>package k8snotifier\n\nimport (\n    \"context\"\n    \"fmt\"\n\n    corev1 \"k8s.io/api/core/v1\"\n    metav1 \"k8s.io/apimachinery/pkg/apis/meta/v1\"\n    \"k8s.io/client-go/kubernetes\"\n)\n\ntype K8sNotifier struct {\n    client *kubernetes.Clientset\n}\n\nfunc New(client *kubernetes.Clientset) *K8sNotifier {\n    return &amp;K8sNotifier{client: client}\n}\n\nfunc (n *K8sNotifier) Name() string {\n    return \"kubernetes\"\n}\n\nfunc (n *K8sNotifier) Cordon(ctx context.Context, nodeID string, reason string) error {\n    node, err := n.client.CoreV1().Nodes().Get(ctx, nodeID, metav1.GetOptions{})\n    if err != nil {\n        return fmt.Errorf(\"get node: %w\", err)\n    }\n\n    node.Spec.Unschedulable = true\n    _, err = n.client.CoreV1().Nodes().Update(ctx, node, metav1.UpdateOptions{})\n    return err\n}\n\nfunc (n *K8sNotifier) Uncordon(ctx context.Context, nodeID string) error {\n    node, err := n.client.CoreV1().Nodes().Get(ctx, nodeID, metav1.GetOptions{})\n    if err != nil {\n        return fmt.Errorf(\"get node: %w\", err)\n    }\n\n    node.Spec.Unschedulable = false\n    _, err = n.client.CoreV1().Nodes().Update(ctx, node, metav1.UpdateOptions{})\n    return err\n}\n\nfunc (n *K8sNotifier) Drain(ctx context.Context, nodeID string, reason string) error {\n    // List pods on the node\n    pods, err := n.client.CoreV1().Pods(\"\").List(ctx, metav1.ListOptions{\n        FieldSelector: \"spec.nodeName=\" + nodeID,\n    })\n    if err != nil {\n        return fmt.Errorf(\"list pods: %w\", err)\n    }\n\n    // Evict each pod\n    for _, pod := range pods.Items {\n        if pod.Namespace == \"kube-system\" {\n            continue // Skip system pods\n        }\n        eviction := &amp;policyv1.Eviction{\n            ObjectMeta: metav1.ObjectMeta{\n                Name:      pod.Name,\n                Namespace: pod.Namespace,\n            },\n        }\n        n.client.CoreV1().Pods(pod.Namespace).Evict(ctx, eviction)\n    }\n    return nil\n}\n\nfunc (n *K8sNotifier) IsDrained(ctx context.Context, nodeID string) (bool, error) {\n    pods, err := n.client.CoreV1().Pods(\"\").List(ctx, metav1.ListOptions{\n        FieldSelector: \"spec.nodeName=\" + nodeID,\n    })\n    if err != nil {\n        return false, fmt.Errorf(\"list pods: %w\", err)\n    }\n\n    // Check if only system pods remain\n    for _, pod := range pods.Items {\n        if pod.Namespace != \"kube-system\" &amp;&amp; pod.Status.Phase == corev1.PodRunning {\n            return false, nil\n        }\n    }\n    return true, nil\n}\n</code></pre>"},{"location":"extending/#example-slurm-notifier","title":"Example: Slurm notifier","text":"<pre><code>package slurmnotifier\n\nimport (\n    \"context\"\n    \"fmt\"\n    \"os/exec\"\n    \"strings\"\n)\n\ntype SlurmNotifier struct{}\n\nfunc New() *SlurmNotifier {\n    return &amp;SlurmNotifier{}\n}\n\nfunc (n *SlurmNotifier) Name() string {\n    return \"slurm\"\n}\n\nfunc (n *SlurmNotifier) Cordon(ctx context.Context, nodeID string, reason string) error {\n    // Set node to drain state (no new jobs)\n    cmd := exec.CommandContext(ctx, \"scontrol\", \"update\", \"nodename=\"+nodeID, \"state=drain\", \"reason=\"+reason)\n    return cmd.Run()\n}\n\nfunc (n *SlurmNotifier) Uncordon(ctx context.Context, nodeID string) error {\n    cmd := exec.CommandContext(ctx, \"scontrol\", \"update\", \"nodename=\"+nodeID, \"state=resume\")\n    return cmd.Run()\n}\n\nfunc (n *SlurmNotifier) Drain(ctx context.Context, nodeID string, reason string) error {\n    // Slurm drain state already prevents new jobs; jobs finish naturally\n    return n.Cordon(ctx, nodeID, reason)\n}\n\nfunc (n *SlurmNotifier) IsDrained(ctx context.Context, nodeID string) (bool, error) {\n    // Check if any jobs are running on this node\n    cmd := exec.CommandContext(ctx, \"squeue\", \"-w\", nodeID, \"-h\", \"-o\", \"%i\")\n    out, err := cmd.Output()\n    if err != nil {\n        return false, fmt.Errorf(\"squeue: %w\", err)\n    }\n    return strings.TrimSpace(string(out)) == \"\", nil\n}\n</code></pre>"},{"location":"extending/#built-in-notifiers","title":"Built-in notifiers","text":"<p>Navarch includes two built-in notifiers:</p> Notifier Description <code>noop</code> Logs operations but takes no action. Default when no notifier is configured. <code>webhook</code> Sends HTTP requests to your workload system. See configuration. <p>See <code>pkg/notifier/</code> for implementation details.</p>"},{"location":"extending/#testing-extensions","title":"Testing extensions","text":""},{"location":"extending/#unit-tests","title":"Unit tests","text":"<pre><code>func TestCostAwareScaler_OffPeak(t *testing.T) {\n    scaler := costscaler.New(9, 18, 80.0, 2)\n\n    state := pool.PoolState{\n        CurrentNodes: 10,\n        MinNodes:     1,\n        MaxNodes:     20,\n        Metrics:      pool.PoolMetrics{Utilization: 50.0},\n    }\n\n    // Test at 3am (off-peak)\n    rec, err := scaler.Recommend(context.Background(), state)\n    require.NoError(t, err)\n    assert.Equal(t, 2, rec.TargetNodes) // Should scale to off-peak minimum\n}\n</code></pre>"},{"location":"extending/#integration-tests-with-simulator","title":"Integration tests with simulator","text":"<p>Test your extensions with realistic scenarios:</p> <pre><code>name: test-custom-autoscaler\nfleet:\n  - id: node-1\n    provider: fake\n    gpu_count: 8\n  - id: node-2\n    provider: fake\n    gpu_count: 8\n\nevents:\n  - at: 0s\n    action: start_fleet\n  - at: 30s\n    action: assert\n    target: node-1\n    params:\n      expected_status: active\n</code></pre>"},{"location":"extending/#best-practices","title":"Best practices","text":"<ol> <li> <p>Handle context cancellation: Check <code>ctx.Done()</code> in long-running operations.</p> </li> <li> <p>Return wrapped errors: Use <code>fmt.Errorf(\"operation: %w\", err)</code> for debuggable errors.</p> </li> <li> <p>Log at appropriate levels: Debug for verbose details, Info for operations, Error for failures.</p> </li> <li> <p>Test with the simulator: Validate behavior before deploying.</p> </li> <li> <p>Document configuration options: Make your extension easy to configure.</p> </li> </ol>"},{"location":"getting-started/","title":"Getting started","text":"<p>This guide walks you through setting up Navarch for local development and testing.</p>"},{"location":"getting-started/#prerequisites","title":"Prerequisites","text":"<ul> <li>Go 1.21 or later.</li> <li>Git for cloning the repository.</li> </ul>"},{"location":"getting-started/#installation","title":"Installation","text":"<p>Clone and build Navarch:</p> <pre><code>git clone https://github.com/NavarchProject/navarch.git\ncd navarch\nmake\n</code></pre> <p>This builds the following binaries in <code>bin/</code>:</p> <ul> <li><code>bin/control-plane</code> - The central management server.</li> <li><code>bin/navarch</code> - The command-line interface.</li> <li><code>bin/node</code> - The node agent (runs on GPU instances).</li> <li><code>bin/simulator</code> - A testing tool for simulating GPU fleets.</li> </ul>"},{"location":"getting-started/#quick-start-with-fake-provider","title":"Quick start with fake provider","text":"<p>The fake provider simulates GPU instances without cloud costs. Use it for local development and testing.</p>"},{"location":"getting-started/#step-1-create-a-configuration-file","title":"Step 1: Create a configuration file","text":"<p>Create <code>config.yaml</code>:</p> <pre><code>server:\n  address: \":50051\"\n  autoscale_interval: 10s\n\nproviders:\n  fake:\n    type: fake\n    gpu_count: 8\n\npools:\n  dev:\n    provider: fake\n    instance_type: gpu_8x_h100\n    region: local\n    min_nodes: 2\n    max_nodes: 5\n    cooldown: 10s\n    autoscaling:\n      type: reactive\n      scale_up_at: 80\n      scale_down_at: 20\n    health:\n      unhealthy_after: 2\n      auto_replace: true\n</code></pre>"},{"location":"getting-started/#step-2-start-the-control-plane","title":"Step 2: Start the control plane","text":"<pre><code>./bin/control-plane --config config.yaml\n</code></pre> <p>The control plane starts and provisions two fake nodes (the <code>min_nodes</code> value).</p>"},{"location":"getting-started/#step-3-list-nodes","title":"Step 3: List nodes","text":"<p>In a new terminal:</p> <pre><code>./bin/navarch list\n</code></pre> <p>Output:</p> <pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 Node ID   \u2502 Provider \u2502 Region \u2502 Zone \u2502 Instance Type \u2502 Status \u2502 Health  \u2502 Last Heartbeat \u2502 GPUs \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 fake-1    \u2502 fake     \u2502 local  \u2502      \u2502 gpu_8x_h100   \u2502 Active \u2502 Healthy \u2502 5s ago         \u2502 8    \u2502\n\u2502 fake-2    \u2502 fake     \u2502 local  \u2502      \u2502 gpu_8x_h100   \u2502 Active \u2502 Healthy \u2502 5s ago         \u2502 8    \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"getting-started/#step-4-manage-nodes","title":"Step 4: Manage nodes","text":"<p>To cordon a node (prevent new workloads):</p> <pre><code>./bin/navarch cordon fake-1\n</code></pre> <p>To drain a node (evict workloads and cordon):</p> <pre><code>./bin/navarch drain fake-1\n</code></pre> <p>To view node details:</p> <pre><code>./bin/navarch get fake-1\n</code></pre>"},{"location":"getting-started/#next-steps","title":"Next steps","text":"<p>To connect real cloud providers, see the configuration reference.</p> <p>To learn about autoscaling strategies, see pool management.</p> <p>To deploy in production, see the deployment guide.</p>"},{"location":"getting-started/#using-the-simulator","title":"Using the simulator","text":"<p>The simulator tests Navarch behavior without running the full system. It uses scenario files to define fleet configurations and events.</p>"},{"location":"getting-started/#run-a-scenario","title":"Run a scenario","text":"<pre><code>./bin/simulator run scenarios/basic-fleet.yaml\n</code></pre>"},{"location":"getting-started/#interactive-mode","title":"Interactive mode","text":"<p>Run the simulator in interactive mode to test CLI commands:</p> <pre><code>./bin/simulator interactive scenarios/basic-fleet.yaml\n</code></pre> <p>Then use the CLI in another terminal:</p> <pre><code>./bin/navarch -s http://localhost:8080 list\n</code></pre> <p>For more information, see simulator documentation.</p>"},{"location":"getting-started/#troubleshooting","title":"Troubleshooting","text":""},{"location":"getting-started/#connection-refused","title":"Connection refused","text":"<p>If <code>navarch list</code> returns \"connection refused\":</p> <ol> <li>Verify the control plane is running.</li> <li>Check the address matches (default is <code>http://localhost:50051</code>).</li> <li>Use the <code>-s</code> flag to specify a different address.</li> </ol>"},{"location":"getting-started/#no-nodes-appear","title":"No nodes appear","text":"<p>If nodes do not appear after starting the control plane:</p> <ol> <li>Check the control plane logs for errors.</li> <li>Verify the pool configuration has <code>min_nodes</code> greater than zero.</li> <li>Confirm the provider is configured correctly.</li> </ol>"},{"location":"getting-started/#build-errors","title":"Build errors","text":"<p>If the build fails:</p> <ol> <li>Verify Go 1.21 or later is installed: <code>go version</code></li> <li>Run <code>go mod download</code> to fetch dependencies.</li> <li>Check for missing dependencies if building with GPU support.</li> </ol>"},{"location":"health-policy/","title":"Health Policy","text":"<p>Navarch uses CEL (Common Expression Language) to evaluate GPU health events and determine node health status. You can customize this logic by providing a health policy file.</p> <p>If no policy is specified, Navarch uses a built-in default policy that classifies fatal XID errors (like XID 79 \"GPU has fallen off the bus\") as unhealthy and recoverable errors as degraded.</p>"},{"location":"health-policy/#enabling-a-custom-policy","title":"Enabling a custom policy","text":"<p>Reference the policy file in your configuration:</p> <pre><code>server:\n  health_policy: ./health-policy.yaml\n</code></pre>"},{"location":"health-policy/#policy-file-format","title":"Policy file format","text":"<pre><code>version: v1\n\nmetadata:\n  name: my-policy\n  description: Custom health policy for my fleet\n\nrules:\n  # More specific rules first\n  - name: fatal-xid\n    description: XID errors indicating unrecoverable GPU failure\n    condition: |\n      event.event_type == \"xid\" &amp;&amp; event.metrics.xid_code in [48, 79, 95]\n    result: unhealthy\n\n  - name: recoverable-xid\n    description: XID errors that may recover\n    condition: event.event_type == \"xid\"\n    result: degraded\n\n  - name: thermal-critical\n    condition: |\n      event.event_type == \"thermal\" &amp;&amp;\n      event.metrics.temperature &gt;= 95\n    result: unhealthy\n\n  # Default rule must be last\n  - name: default\n    condition: \"true\"\n    result: healthy\n</code></pre> <p>Rules are evaluated in order; the first matching rule determines the result. Place more specific rules before general ones, and always include a default rule at the end.</p>"},{"location":"health-policy/#rule-fields","title":"Rule fields","text":"Field Required Description <code>name</code> Yes Unique rule identifier <code>description</code> No Human-readable description <code>condition</code> Yes CEL expression that returns true when rule matches <code>result</code> Yes Result when rule matches: <code>healthy</code>, <code>degraded</code>, or <code>unhealthy</code>"},{"location":"health-policy/#cel-event-fields","title":"CEL event fields","text":"<p>The following fields are available in CEL expressions:</p> Field Type Description <code>event.event_type</code> string Event type: <code>xid</code>, <code>thermal</code>, <code>ecc_dbe</code>, <code>ecc_sbe</code>, <code>nvlink</code>, <code>pcie</code>, <code>power</code> <code>event.system</code> string DCGM health watch system identifier <code>event.gpu_index</code> int GPU index (0-based, -1 for node-level) <code>event.metrics</code> map Event-specific metrics <code>event.message</code> string Human-readable description <p>Common metrics by event type:</p> Event Type Metric Type Description <code>xid</code> <code>xid_code</code> int NVIDIA XID error code <code>thermal</code> <code>temperature</code> int GPU temperature in Celsius <code>ecc_dbe</code> <code>ecc_dbe_count</code> int Double-bit ECC error count <code>ecc_sbe</code> <code>ecc_sbe_count</code> int Single-bit ECC error count"},{"location":"health-policy/#example-policies","title":"Example policies","text":""},{"location":"health-policy/#strict-policy","title":"Strict policy","text":"<p>Treat all XID errors as fatal:</p> <pre><code>rules:\n  - name: any-xid-fatal\n    condition: event.event_type == \"xid\"\n    result: unhealthy\n  - name: default\n    condition: \"true\"\n    result: healthy\n</code></pre>"},{"location":"health-policy/#permissive-policy","title":"Permissive policy","text":"<p>Only fail on the most severe errors:</p> <pre><code>rules:\n  - name: bus-error-only\n    condition: |\n      event.event_type == \"xid\" &amp;&amp; event.metrics.xid_code == 79\n    result: unhealthy\n  - name: default\n    condition: \"true\"\n    result: healthy\n</code></pre>"},{"location":"health-policy/#gpu-specific-policy","title":"GPU-specific policy","text":"<p>Different thresholds for different GPUs:</p> <pre><code>rules:\n  - name: gpu0-strict\n    description: GPU 0 is critical, any error is fatal\n    condition: event.gpu_index == 0 &amp;&amp; event.event_type == \"xid\"\n    result: unhealthy\n  - name: other-gpu-permissive\n    condition: event.event_type == \"xid\"\n    result: degraded\n  - name: default\n    condition: \"true\"\n    result: healthy\n</code></pre>"},{"location":"health-policy/#testing-policies","title":"Testing policies","text":"<p>Use the simulator to test health policies before deploying to production. The simulator HTML report includes a \"Policy Rules\" section showing which rules matched for each failure.</p> <pre><code>./bin/simulator run scenarios/xid-classification.yaml -v\n</code></pre> <p>See Health Monitoring for background on health events and XID errors.</p>"},{"location":"metrics/","title":"Metrics and monitoring","text":"<p>Navarch collects metrics from GPU nodes to enable autoscaling and health monitoring.</p>"},{"location":"metrics/#metrics-collection","title":"Metrics collection","text":""},{"location":"metrics/#what-is-collected","title":"What is collected","text":"<p>Every heartbeat (5-30 seconds) includes:</p> <p>Node-level metrics:</p> <ul> <li>CPU usage percentage</li> <li>Memory usage percentage</li> <li>Timestamp</li> </ul> <p>Per-GPU metrics:</p> <ul> <li>GPU index</li> <li>Utilization percentage (0-100)</li> <li>Temperature in Celsius</li> <li>Power usage in watts</li> <li>Memory used in bytes</li> </ul> <p>Health status:</p> <ul> <li>Boot check results</li> <li>GPU communication status</li> <li>Health event detection (XID errors, thermal, ECC)</li> </ul>"},{"location":"metrics/#collection-flow","title":"Collection flow","text":"<pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510     Heartbeat      \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 Node Agent  \u2502 \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500&gt;\u2502 Control Plane\u2502\n\u2502             \u2502   (every 5-30s)     \u2502              \u2502\n\u2502 - Query GPU \u2502                     \u2502 - Store      \u2502\n\u2502 - Collect   \u2502                     \u2502 - Aggregate  \u2502\n\u2502 - Check     \u2502                     \u2502 - Autoscale  \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                     \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"metrics/#node-side-collection","title":"Node-side collection","text":"<p>The node daemon uses the <code>metrics.Collector</code> to gather system and GPU metrics.</p> <p>System metrics are collected from <code>/proc</code> filesystem (Linux): - CPU usage: Calculated from <code>/proc/stat</code> using delta between consecutive reads - Memory usage: Read from <code>/proc/meminfo</code> using <code>MemTotal</code> and <code>MemAvailable</code></p> <p>GPU metrics are collected via the GPU manager interface: - Queries GPU temperature, power, utilization, and memory - Collects health events (XID errors, thermal warnings, ECC errors) - Uses injectable GPU manager for testing/development</p> <p>Code location: <code>pkg/node/metrics/</code></p> <pre><code>// Create a metrics collector\ncollector := metrics.NewCollector(gpuManager, nil)\n\n// Collect all metrics\nnodeMetrics, err := collector.Collect(ctx)\n// Returns: CpuUsagePercent, MemoryUsagePercent, GpuMetrics[]\n</code></pre> <p>Custom system reader: For non-Linux systems or testing, implement <code>SystemMetricsReader</code>:</p> <pre><code>type SystemMetricsReader interface {\n    ReadCPUUsage(ctx context.Context) (float64, error)\n    ReadMemoryUsage(ctx context.Context) (float64, error)\n}\n\n// Use custom reader\ncustomReader := &amp;MyCustomReader{}\ncollector := metrics.NewCollector(gpuManager, customReader)\n</code></pre>"},{"location":"metrics/#storage-and-retention","title":"Storage and retention","text":"<p>Metrics are stored in-memory per node: - Up to 100 samples per node - Oldest samples automatically pruned - Query window: last 5 minutes (default)</p> <p>For production deployments requiring longer retention, implement a custom database backend.</p>"},{"location":"metrics/#metrics-aggregation","title":"Metrics aggregation","text":""},{"location":"metrics/#pool-level-aggregation","title":"Pool-level aggregation","text":"<p>The control plane aggregates metrics by pool for autoscaling decisions.</p> <p>Current utilization: Average GPU utilization across all GPUs in the pool.</p> <p>Example: Pool has 2 nodes with 8 GPUs each (16 total GPUs): - Node 1 GPUs: 80%, 90%, 75%, 85%, 70%, 80%, 85%, 75% - Node 2 GPUs: 60%, 70%, 65%, 55%, 70%, 65%, 60%, 75%</p> <p>Pool utilization = (80+90+75+85+70+80+85+75+60+70+65+55+70+65+60+75) / 16 = 71.25%</p> <p>Utilization history: Per-node average utilization for the last 5 minutes. Used for trend analysis by predictive autoscalers.</p>"},{"location":"metrics/#pool-filtering","title":"Pool filtering","text":"<p>Nodes are assigned to pools via labels:</p> <pre><code>pools:\n  training:\n    labels:\n      pool: training\n      team: ml-research\n</code></pre> <p>The <code>pool</code> label is automatically set and used for metrics aggregation. Additional labels are for organization and filtering.</p>"},{"location":"metrics/#autoscaling-metrics","title":"Autoscaling metrics","text":"<p>Different autoscaler types use different metrics.</p>"},{"location":"metrics/#reactive-autoscaler","title":"Reactive autoscaler","text":"<p>Uses current GPU utilization:</p> <pre><code>autoscaling:\n  type: reactive\n  scale_up_at: 75    # Scale up when utilization &gt; 75%\n  scale_down_at: 25  # Scale down when utilization &lt; 25%\n</code></pre> <p>Evaluation: Every 30 seconds (configurable via <code>autoscale_interval</code>)</p> <p>Example: - Current: 3 nodes, 85% GPU utilization - Recommendation: Scale up to 4 nodes (utilization &gt; 75%) - After cooldown: Actually provision 4th node</p>"},{"location":"metrics/#queue-based-autoscaler","title":"Queue-based autoscaler","text":"<p>Uses job queue depth:</p> <pre><code>autoscaling:\n  type: queue\n  jobs_per_node: 10\n</code></pre> <p>Requires: External scheduler integration via <code>MetricsSource</code> interface.</p> <p>Example: - Current: 5 nodes, 73 jobs in queue - Calculation: ceil(73 / 10) = 8 nodes needed - Recommendation: Scale up to 8 nodes</p>"},{"location":"metrics/#scheduled-autoscaler","title":"Scheduled autoscaler","text":"<p>Does not use metrics. Scales based on time:</p> <pre><code>autoscaling:\n  type: scheduled\n  schedule:\n    - days: [monday, tuesday, wednesday, thursday, friday]\n      start: 9\n      end: 18\n      min_nodes: 10\n      max_nodes: 50\n</code></pre> <p>Evaluation: Checks current time and applies appropriate limits.</p>"},{"location":"metrics/#predictive-autoscaler","title":"Predictive autoscaler","text":"<p>Uses utilization history for trend analysis:</p> <pre><code>autoscaling:\n  type: predictive\n  lookback_window: 10  # Samples to analyze\n  growth_factor: 1.2   # Proactive scaling multiplier\n</code></pre> <p>Analyzes recent utilization trend and scales preemptively.</p>"},{"location":"metrics/#composite-autoscaler","title":"Composite autoscaler","text":"<p>Combines multiple metrics:</p> <pre><code>autoscaling:\n  type: composite\n  mode: max  # Take maximum recommendation\n  autoscalers:\n    - type: reactive\n      scale_up_at: 80\n      scale_down_at: 20\n    - type: queue\n      jobs_per_node: 10\n</code></pre> <p>Use case: Scale based on both GPU utilization and job queue depth, whichever demands more capacity.</p>"},{"location":"metrics/#integrating-external-schedulers","title":"Integrating external schedulers","text":"<p>To provide queue depth and pending job metrics, implement the <code>MetricsSource</code> interface.</p>"},{"location":"metrics/#interface","title":"Interface","text":"<pre><code>type MetricsSource interface {\n    GetPoolMetrics(ctx context.Context, poolName string) (*PoolMetrics, error)\n}\n\ntype PoolMetrics struct {\n    Utilization        float64   // GPU utilization (provided by Navarch)\n    PendingJobs        int       // Jobs waiting to start\n    QueueDepth         int       // Pending + running jobs\n    UtilizationHistory []float64 // Historical utilization\n}\n</code></pre>"},{"location":"metrics/#example-kubernetes-integration","title":"Example: Kubernetes integration","text":"<pre><code>package main\n\nimport (\n    \"context\"\n    \"github.com/NavarchProject/navarch/pkg/controlplane\"\n    metav1 \"k8s.io/apimachinery/pkg/apis/meta/v1\"\n    \"k8s.io/client-go/kubernetes\"\n)\n\ntype KubernetesMetrics struct {\n    clientset *kubernetes.Clientset\n    dbMetrics *controlplane.DBMetricsSource\n}\n\nfunc (k *KubernetesMetrics) GetPoolMetrics(ctx context.Context, poolName string) (*controlplane.PoolMetrics, error) {\n    // Get GPU utilization from Navarch's built-in metrics\n    baseMetrics, err := k.dbMetrics.GetPoolMetrics(ctx, poolName)\n    if err != nil {\n        return nil, err\n    }\n\n    // Query Kubernetes for pods with pool label\n    pods, err := k.clientset.CoreV1().Pods(\"\").List(ctx, metav1.ListOptions{\n        LabelSelector: \"pool=\" + poolName,\n    })\n    if err != nil {\n        return nil, err\n    }\n\n    // Count pending and running pods\n    var pending, running int\n    for _, pod := range pods.Items {\n        if pod.Status.Phase == \"Pending\" {\n            pending++\n        } else if pod.Status.Phase == \"Running\" {\n            running++\n        }\n    }\n\n    // Combine metrics\n    baseMetrics.PendingJobs = pending\n    baseMetrics.QueueDepth = pending + running\n\n    return baseMetrics, nil\n}\n</code></pre> <p>Then use it:</p> <pre><code>k8sMetrics := &amp;KubernetesMetrics{\n    clientset: clientset,\n    dbMetrics: controlplane.NewDBMetricsSource(database, logger),\n}\n\npoolManager := controlplane.NewPoolManager(cfg, k8sMetrics, logger)\n</code></pre>"},{"location":"metrics/#example-slurm-integration","title":"Example: Slurm integration","text":"<pre><code>type SlurmMetrics struct {\n    slurmHost string\n    dbMetrics *controlplane.DBMetricsSource\n}\n\nfunc (s *SlurmMetrics) GetPoolMetrics(ctx context.Context, poolName string) (*controlplane.PoolMetrics, error) {\n    baseMetrics, _ := s.dbMetrics.GetPoolMetrics(ctx, poolName)\n\n    // Query Slurm via scontrol or sacct\n    output, _ := exec.CommandContext(ctx, \"squeue\", \n        \"-h\", \"-p\", poolName, \"-t\", \"PENDING\", \"-o\", \"%i\").Output()\n\n    pending := len(strings.Split(string(output), \"\\n\")) - 1\n\n    output, _ = exec.CommandContext(ctx, \"squeue\",\n        \"-h\", \"-p\", poolName, \"-t\", \"RUNNING\", \"-o\", \"%i\").Output()\n\n    running := len(strings.Split(string(output), \"\\n\")) - 1\n\n    baseMetrics.PendingJobs = pending\n    baseMetrics.QueueDepth = pending + running\n\n    return baseMetrics, nil\n}\n</code></pre>"},{"location":"metrics/#monitoring-and-observability","title":"Monitoring and observability","text":""},{"location":"metrics/#prometheus-metrics","title":"Prometheus metrics","text":"<p>The control plane exposes Prometheus metrics at <code>/metrics</code>.</p> <p>Available metrics:</p> Metric Labels Description <code>navarch_nodes_total</code> <code>status</code> Total number of nodes by status (active, cordoned, draining, unhealthy) <code>navarch_node_health_status</code> <code>node_id</code>, <code>status</code> Health status per node (1=healthy, 0.5=degraded, 0=unhealthy) <code>navarch_gpus_total</code> <code>provider</code> Total number of GPUs by provider"},{"location":"metrics/#structured-logging","title":"Structured logging","text":"<p>All components emit structured JSON logs with: - <code>level</code>: INFO, WARN, ERROR - <code>msg</code>: Human-readable message - <code>time</code>: RFC3339 timestamp - Context fields (pool, node_id, error, etc.)</p> <p>Example:</p> <pre><code>{\n  \"time\": \"2026-01-19T22:00:15Z\",\n  \"level\": \"INFO\",\n  \"msg\": \"scaling up\",\n  \"pool\": \"training\",\n  \"from\": 5,\n  \"to\": 8,\n  \"reason\": \"utilization 87.3% &gt; 75.0% threshold\"\n}\n</code></pre>"},{"location":"metrics/#health-endpoints","title":"Health endpoints","text":"<p>Liveness probe: <code>GET /healthz</code> - Returns 200 if control plane is running - Use for container health checks</p> <p>Readiness probe: <code>GET /readyz</code> - Returns 200 if database is accessible - Returns 503 if not ready - Use for load balancer health checks</p>"},{"location":"metrics/#metrics-api","title":"Metrics API","text":"<p>Query metrics via gRPC API (future work):</p> <pre><code>service MetricsService {\n  rpc GetNodeMetrics(GetNodeMetricsRequest) returns (NodeMetricsResponse);\n  rpc GetPoolMetrics(GetPoolMetricsRequest) returns (PoolMetricsResponse);\n  rpc QueryMetrics(QueryMetricsRequest) returns (QueryMetricsResponse);\n}\n</code></pre> <p>Current workaround: Query the in-memory database directly via the CLI or custom tooling.</p>"},{"location":"metrics/#best-practices","title":"Best practices","text":""},{"location":"metrics/#heartbeat-intervals","title":"Heartbeat intervals","text":"<p>Fast (5s): Development and testing Standard (30s): Production with reactive autoscaling Slow (60s): Large clusters (1000+ nodes) to reduce control plane load</p>"},{"location":"metrics/#autoscale-intervals","title":"Autoscale intervals","text":"<p>Fast (10s): Development and testing Standard (30s): Production with quick response times Slow (60-120s): Cost-sensitive workloads to avoid over-provisioning</p>"},{"location":"metrics/#cooldown-periods","title":"Cooldown periods","text":"<p>Short (2-3m): Development and testing Standard (5m): Production to prevent oscillation Long (10-15m): Large nodes (expensive to provision/terminate)</p>"},{"location":"metrics/#metrics-retention","title":"Metrics retention","text":"<p>The default 100 samples per node retains approximately: - 8 minutes at 5s heartbeat interval - 50 minutes at 30s heartbeat interval</p> <p>For longer retention, implement custom database backend or export to time-series database.</p>"},{"location":"metrics/#troubleshooting","title":"Troubleshooting","text":""},{"location":"metrics/#no-metrics-for-pool","title":"No metrics for pool","text":"<p>Symptom: Autoscaler always reports 0% utilization.</p> <p>Causes:</p> <ol> <li>Nodes not registered with pool label</li> <li>Nodes not sending metrics in heartbeats</li> <li>Pool name mismatch</li> </ol> <p>Debug: <pre><code># Check node labels\nnavarch get node-1 --output json | jq .metadata.labels\n\n# Verify pool name in config\ngrep \"pools:\" config.yaml -A 5\n\n# Check control plane logs\ngrep \"failed to get metrics\" logs.json\n</code></pre></p>"},{"location":"metrics/#autoscaler-not-scaling","title":"Autoscaler not scaling","text":"<p>Symptom: Utilization high but no scale up.</p> <p>Causes:</p> <ol> <li>Cooldown period active</li> <li>At max nodes limit</li> <li>Provider provisioning failures</li> </ol> <p>Debug: <pre><code># Check pool status\nnavarch list\n\n# Look for scaling events\ngrep \"scaling\" logs.json | tail -20\n\n# Check cooldown\ngrep \"cooldown active\" logs.json\n</code></pre></p>"},{"location":"metrics/#high-control-plane-memory","title":"High control plane memory","text":"<p>Symptom: Control plane memory usage growing.</p> <p>Causes:</p> <ol> <li>Metrics retention with many nodes (100 samples \u00d7 number of nodes)</li> <li>Memory leak (report bug)</li> </ol> <p>Solutions:</p> <ol> <li>Reduce metrics retention (requires code change)</li> <li>Restart control plane periodically</li> <li>Implement external database backend</li> </ol>"},{"location":"pool-management/","title":"Pool management","text":"<p>This guide covers how to configure and manage GPU node pools with Navarch, including autoscaling strategies and health-based replacement.</p> <p>For detailed information on autoscaling strategies and metrics, see architecture and metrics.</p> <p>For the complete configuration reference, see the configuration reference.</p>"},{"location":"pool-management/#overview","title":"Overview","text":"<p>A pool is a group of GPU nodes that share the same:</p> <ul> <li>Cloud provider (Lambda, GCP, AWS)</li> <li>Instance type</li> <li>Region</li> <li>Scaling limits and autoscaler configuration</li> <li>Health policies</li> </ul> <p>Pools enable you to manage heterogeneous GPU resources with independent scaling policies. For example, you might have separate pools for training workloads (large H100 instances with conservative scaling) and inference (smaller A100 instances with aggressive autoscaling).</p>"},{"location":"pool-management/#configuration","title":"Configuration","text":"<p>Pools are configured via a YAML file passed to the control plane:</p> <pre><code>control-plane --config pools.yaml\n</code></pre>"},{"location":"pool-management/#basic-pool-configuration","title":"Basic pool configuration","text":"<pre><code>pools:\n  - name: training\n    provider: lambda\n    instance_type: gpu_8x_h100_sxm5\n    region: us-west-2\n\n    scaling:\n      min_nodes: 2\n      max_nodes: 20\n      cooldown_period: 5m\n\n      autoscaler:\n        type: reactive\n        scale_up_threshold: 80\n        scale_down_threshold: 20\n\n    health:\n      unhealthy_threshold: 2\n      auto_replace: true\n\n    labels:\n      workload: training\n</code></pre>"},{"location":"pool-management/#configuration-reference","title":"Configuration reference","text":"Field Description Required <code>name</code> Unique pool identifier Yes <code>provider</code> Cloud provider: <code>lambda</code>, <code>gcp</code>, <code>aws</code> Yes <code>instance_type</code> Provider-specific instance type Yes <code>region</code> Cloud region Yes <code>zones</code> List of availability zones for multi-zone pools No <code>scaling.min_nodes</code> Minimum nodes to maintain Yes <code>scaling.max_nodes</code> Maximum nodes allowed Yes <code>scaling.cooldown_period</code> Time between scaling actions (e.g., <code>5m</code>) No <code>health.unhealthy_threshold</code> Consecutive failures before node is unhealthy No <code>health.auto_replace</code> Automatically replace unhealthy nodes No <code>labels</code> Key-value labels for workload routing No"},{"location":"pool-management/#autoscaler-strategies","title":"Autoscaler strategies","text":"<p>Navarch supports multiple autoscaling strategies that you can configure per pool.</p>"},{"location":"pool-management/#reactive-autoscaler","title":"Reactive autoscaler","text":"<p>Scales based on current GPU utilization. Use this for workloads with steady demand patterns.</p> <pre><code>autoscaler:\n  type: reactive\n  scale_up_threshold: 80    # Scale up when utilization &gt; 80%\n  scale_down_threshold: 20  # Scale down when utilization &lt; 20%\n</code></pre>"},{"location":"pool-management/#queue-based-autoscaler","title":"Queue-based autoscaler","text":"<p>Scales based on job queue depth. Use this for batch processing workloads where you want to clear the queue quickly.</p> <pre><code>autoscaler:\n  type: queue\n  jobs_per_node: 100  # Target 100 jobs per node\n</code></pre>"},{"location":"pool-management/#scheduled-autoscaler","title":"Scheduled autoscaler","text":"<p>Adjusts scaling limits based on time of day or day of week. Use this for predictable demand patterns.</p> <pre><code>autoscaler:\n  type: scheduled\n  schedule:\n    # Business hours: larger capacity\n    - days: [monday, tuesday, wednesday, thursday, friday]\n      start_hour: 9\n      end_hour: 18\n      min_nodes: 10\n      max_nodes: 100\n    # Weekends: minimal capacity\n    - days: [saturday, sunday]\n      start_hour: 0\n      end_hour: 24\n      min_nodes: 0\n      max_nodes: 10\n  # Fall back to reactive scaling within the adjusted limits\n  fallback:\n    type: reactive\n    scale_up_threshold: 80\n    scale_down_threshold: 20\n</code></pre>"},{"location":"pool-management/#predictive-autoscaler","title":"Predictive autoscaler","text":"<p>Uses historical utilization data to anticipate demand. Use this for workloads with gradual ramp-up patterns.</p> <pre><code>autoscaler:\n  type: predictive\n  lookback_window: 30    # Analyze last 30 utilization samples\n  growth_factor: 1.5     # Scale 1.5x the predicted need\n  fallback:\n    type: reactive\n    scale_up_threshold: 70\n    scale_down_threshold: 30\n</code></pre>"},{"location":"pool-management/#composite-autoscaler","title":"Composite autoscaler","text":"<p>Combines multiple strategies for complex scenarios. The mode determines how recommendations are combined.</p> <pre><code>autoscaler:\n  type: composite\n  mode: max  # Options: max, min, avg\n  autoscalers:\n    - type: reactive\n      scale_up_threshold: 70\n      scale_down_threshold: 30\n    - type: queue\n      jobs_per_node: 50\n</code></pre> <p>Modes:</p> <ul> <li><code>max</code>: Take the highest recommendation (most aggressive scaling)</li> <li><code>min</code>: Take the lowest recommendation (most conservative)</li> <li><code>avg</code>: Average all recommendations</li> </ul>"},{"location":"pool-management/#provider-configuration","title":"Provider configuration","text":"<p>Providers require credentials to provision instances.</p>"},{"location":"pool-management/#fake-provider-local-development","title":"Fake provider (local development)","text":"<p>The fake provider simulates cloud instances by running node agents as goroutines. Use this for local development and testing without cloud costs:</p> <pre><code>pools:\n  - name: dev-pool\n    provider: fake\n    instance_type: gpu_8x_h100\n    region: local\n    # ... scaling and health config\n\nproviders:\n  fake:\n    gpu_count: 8  # GPUs per fake instance\n</code></pre> <p>No credentials are required. Each provisioned \"instance\" spawns a goroutine that:</p> <ul> <li>Registers with the control plane.</li> <li>Sends heartbeats and health check results.</li> <li>Responds to commands (cordon, drain).</li> </ul> <p>See <code>examples/pools-dev.yaml</code> for a complete local development configuration.</p>"},{"location":"pool-management/#lambda-labs","title":"Lambda Labs","text":"<pre><code>providers:\n  lambda:\n    api_key_secret: navarch/lambda-api-key\n</code></pre> <p>Set the API key via environment variable:</p> <pre><code>export LAMBDA_API_KEY=your-api-key\n</code></pre>"},{"location":"pool-management/#gcp","title":"GCP","text":"<pre><code>providers:\n  gcp:\n    project: my-gcp-project\n    credentials_secret: navarch/gcp-credentials\n</code></pre>"},{"location":"pool-management/#aws","title":"AWS","text":"<pre><code>providers:\n  aws:\n    region: us-east-1\n    credentials_secret: navarch/aws-credentials\n</code></pre>"},{"location":"pool-management/#global-settings","title":"Global settings","text":"<p>Apply default settings across all pools:</p> <pre><code>global:\n  ssh_key_names:\n    - ops-team\n    - ml-team\n\n  agent:\n    server: https://control-plane.example.com\n    heartbeat_interval: 30s\n    health_check_interval: 60s\n</code></pre>"},{"location":"pool-management/#health-based-replacement","title":"Health-based replacement","text":"<p>When <code>auto_replace: true</code> is set, Navarch automatically replaces nodes that fail consecutive health checks.</p> <pre><code>health:\n  unhealthy_threshold: 2   # Replace after 2 consecutive failures\n  auto_replace: true\n</code></pre> <p>The replacement process:</p> <ol> <li>Node fails health checks (XID error, thermal event, ECC error, etc.)</li> <li>After <code>unhealthy_threshold</code> consecutive failures, node is marked unhealthy.</li> <li>The control plane notifies the pool manager via the health observer interface.</li> <li>If <code>auto_replace</code> is enabled, the unhealthy node is terminated.</li> <li>A replacement node is provisioned immediately.</li> </ol> <p>This ensures your pools maintain capacity even when GPU hardware fails.</p>"},{"location":"pool-management/#health-recovery","title":"Health recovery","text":"<p>When a node that was previously unhealthy reports a fully healthy status, it automatically transitions back to active. This handles cases where transient errors (like recoverable XID codes) resolve without requiring node replacement.</p> <p>The health state machine:</p> <ul> <li>Active \u2192 Unhealthy: Node reports unhealthy health check results.</li> <li>Unhealthy \u2192 Active: Node reports fully healthy results (not degraded).</li> <li>Unhealthy \u2192 Unhealthy: Node reports degraded results (partial recovery is not sufficient).</li> <li>Unhealthy \u2192 Terminated: Auto-replacement terminates the node (if <code>auto_replace: true</code> and threshold is reached).</li> </ul> <p>Note: Degraded health status keeps a node in the unhealthy state because partial recovery may indicate an underlying issue that could recur. Only a fully healthy status restores the node to active.</p>"},{"location":"pool-management/#scaling-behavior","title":"Scaling behavior","text":""},{"location":"pool-management/#cooldown-period","title":"Cooldown period","text":"<p>The <code>cooldown_period</code> prevents thrashing by requiring a minimum time between scaling actions:</p> <pre><code>scaling:\n  cooldown_period: 5m\n</code></pre> <p>During cooldown, no scaling actions occur even if the autoscaler recommends them.</p>"},{"location":"pool-management/#scaling-limits","title":"Scaling limits","text":"<p><code>min_nodes</code> and <code>max_nodes</code> are hard limits that the autoscaler respects:</p> <ul> <li>Nodes are never scaled below <code>min_nodes</code></li> <li>Nodes are never scaled above <code>max_nodes</code></li> <li>Scale-down prefers removing cordoned nodes first</li> </ul>"},{"location":"pool-management/#manual-scaling","title":"Manual scaling","text":"<p>You can manually scale pools via the CLI:</p> <pre><code># Scale to specific count\nnavarch pool scale training --nodes 10\n\n# View pool status\nnavarch pool status training\n</code></pre>"},{"location":"pool-management/#example-configurations","title":"Example configurations","text":""},{"location":"pool-management/#high-availability-inference","title":"High-availability inference","text":"<pre><code>pools:\n  - name: inference-prod\n    provider: lambda\n    instance_type: gpu_1x_a100_sxm4\n    region: us-east-1\n\n    scaling:\n      min_nodes: 5          # Always have capacity\n      max_nodes: 50\n      cooldown_period: 2m   # React quickly\n\n      autoscaler:\n        type: composite\n        mode: max           # Aggressive scaling\n        autoscalers:\n          - type: reactive\n            scale_up_threshold: 60\n            scale_down_threshold: 30\n          - type: queue\n            jobs_per_node: 100\n\n    health:\n      unhealthy_threshold: 1   # Replace immediately\n      auto_replace: true\n\n    labels:\n      workload: inference\n      tier: production\n</code></pre>"},{"location":"pool-management/#cost-optimized-training","title":"Cost-optimized training","text":"<pre><code>pools:\n  - name: training-batch\n    provider: lambda\n    instance_type: gpu_8x_h100_sxm5\n    region: us-west-2\n\n    scaling:\n      min_nodes: 0          # Scale to zero when idle\n      max_nodes: 20\n      cooldown_period: 10m  # Conservative scaling\n\n      autoscaler:\n        type: scheduled\n        schedule:\n          - days: [monday, tuesday, wednesday, thursday, friday]\n            start_hour: 8\n            end_hour: 20\n            min_nodes: 5\n            max_nodes: 20\n        fallback:\n          type: reactive\n          scale_up_threshold: 90\n          scale_down_threshold: 10\n\n    health:\n      unhealthy_threshold: 3\n      auto_replace: true\n\n    labels:\n      workload: training\n      tier: batch\n</code></pre>"},{"location":"pool-management/#monitoring","title":"Monitoring","text":"<p>The control plane logs all scaling decisions with reasons:</p> <pre><code>INFO scaling up pool=training from=5 to=8 adding=3 reason=\"utilization 85.0% &gt; 80.0% threshold\"\nINFO scaling down pool=inference from=10 to=7 removing=3 reason=\"utilization 15.0% &lt; 20.0% threshold\"\n</code></pre> <p>Pool status is available via the API:</p> <pre><code>curl http://localhost:50051/api/v1/pools/training/status\n</code></pre> <p>Response:</p> <pre><code>{\n  \"name\": \"training\",\n  \"total_nodes\": 8,\n  \"healthy_nodes\": 8,\n  \"unhealthy_nodes\": 0,\n  \"cordoned_nodes\": 0,\n  \"can_scale_up\": true,\n  \"can_scale_down\": true\n}\n</code></pre>"},{"location":"pool-management/#integrating-metrics","title":"Integrating metrics","text":"<p>To provide utilization and queue metrics to the autoscaler, implement the <code>MetricsSource</code> interface:</p> <pre><code>type MetricsSource interface {\n    GetPoolMetrics(ctx context.Context, poolName string) (*PoolMetrics, error)\n}\n\ntype PoolMetrics struct {\n    Utilization        float64   // Average GPU utilization (0-100)\n    PendingJobs        int       // Jobs waiting to be scheduled\n    QueueDepth         int       // Total jobs in queue\n    UtilizationHistory []float64 // For predictive autoscaler\n}\n</code></pre> <p>Connect your workload system (Kubernetes, Ray, custom scheduler) to provide these metrics. Without a metrics source, autoscalers that depend on utilization or queue depth will not scale.</p>"},{"location":"testing-on-gpu/","title":"Testing on real GPUs","text":"<p>This guide describes how to test Navarch on real GPU instances.</p>"},{"location":"testing-on-gpu/#prerequisites","title":"Prerequisites","text":"<p>Before testing on real GPUs, ensure:</p> <ul> <li>All unit tests pass locally: <code>go test ./... -short</code></li> <li>The simulator scenarios pass: <code>./bin/simulator run scenarios/basic-fleet.yaml</code></li> <li>You have access to a GPU cloud provider (Lambda Labs, GCP, AWS)</li> </ul>"},{"location":"testing-on-gpu/#quick-start","title":"Quick start","text":""},{"location":"testing-on-gpu/#1-provision-a-gpu-instance","title":"1. Provision a GPU instance","text":"<p>Lambda Labs: <pre><code># Use Lambda Labs console to create an instance\n# Recommended: gpu_1x_a100_sxm4 or gpu_8x_a100_80gb_sxm4\n</code></pre></p> <p>GCP: <pre><code>gcloud compute instances create navarch-test \\\n  --zone=us-central1-a \\\n  --machine-type=a2-highgpu-1g \\\n  --image-family=ubuntu-2204-lts \\\n  --image-project=ubuntu-os-cloud \\\n  --accelerator=type=nvidia-tesla-a100,count=1 \\\n  --maintenance-policy=TERMINATE\n</code></pre></p> <p>AWS: <pre><code>aws ec2 run-instances \\\n  --image-id ami-0abcdef1234567890 \\\n  --instance-type p4d.24xlarge \\\n  --key-name your-key\n</code></pre></p>"},{"location":"testing-on-gpu/#2-connect-and-verify-gpu","title":"2. Connect and verify GPU","text":"<pre><code>ssh -i your-key.pem ubuntu@&lt;instance-ip&gt;\n\n# Verify NVIDIA driver\nnvidia-smi\n</code></pre>"},{"location":"testing-on-gpu/#3-install-dependencies","title":"3. Install dependencies","text":"<pre><code># Install Go\nwget https://go.dev/dl/go1.22.0.linux-amd64.tar.gz\nsudo tar -C /usr/local -xzf go1.22.0.linux-amd64.tar.gz\necho 'export PATH=$PATH:/usr/local/go/bin' &gt;&gt; ~/.bashrc\nsource ~/.bashrc\n\n# Clone Navarch (or copy from local)\ngit clone https://github.com/NavarchProject/navarch.git\ncd navarch\n</code></pre>"},{"location":"testing-on-gpu/#4-run-tests","title":"4. Run tests","text":"<pre><code># Quick GPU test\n./scripts/test-gpu.sh\n\n# Full end-to-end test\n./scripts/test-e2e.sh\n\n# Stress test (5 minutes)\n./scripts/stress-gpu.sh 300\n</code></pre>"},{"location":"testing-on-gpu/#test-scenarios","title":"Test scenarios","text":""},{"location":"testing-on-gpu/#basic-gpu-detection","title":"Basic GPU detection","text":"<p>Run GPU tests to verify the GPU manager works correctly:</p> <pre><code>go test ./pkg/gpu/... -v\n</code></pre> <p>Expected output: <pre><code>=== RUN   TestInjectable_NewInjectable\n--- PASS: TestInjectable_NewInjectable (0.00s)\n=== RUN   TestInjectable_CollectHealthEvents\n--- PASS: TestInjectable_CollectHealthEvents (0.00s)\n...\n</code></pre></p>"},{"location":"testing-on-gpu/#health-check-validation","title":"Health check validation","text":"<p>Run the node daemon and verify health checks:</p> <pre><code># Terminal 1: Start control plane\ngo run ./cmd/control-plane\n\n# Terminal 2: Start node daemon\ngo run ./cmd/node --node-id test-gpu --provider test\n\n# Terminal 3: Check health after 65 seconds\nsleep 65\ngo run ./cmd/navarch get test-gpu\n</code></pre> <p>Expected output: <pre><code>Health: Healthy\n</code></pre></p>"},{"location":"testing-on-gpu/#health-event-detection","title":"Health event detection","text":"<p>Check for GPU errors in system logs:</p> <pre><code># Check for existing XID errors in dmesg\nsudo dmesg | grep -i \"NVRM: Xid\"\n\n# Run health event tests\ngo test ./pkg/gpu/... -v -run TestInjectable_CollectHealthEvents\n</code></pre>"},{"location":"testing-on-gpu/#command-delivery","title":"Command delivery","text":"<p>Test that commands reach the node:</p> <pre><code># Cordon the node\ngo run ./cmd/navarch cordon test-gpu\n\n# Check node logs for command receipt\ngrep \"received command\" /tmp/node.log\n</code></pre>"},{"location":"testing-on-gpu/#multi-gpu-testing","title":"Multi-GPU testing","text":"<p>For instances with multiple GPUs (e.g., 8x A100):</p> <pre><code># Verify all GPUs detected\ngo run ./cmd/navarch get test-gpu | grep -A 100 \"GPUs:\"\n</code></pre> <p>Expected: All 8 GPUs listed with unique UUIDs.</p>"},{"location":"testing-on-gpu/#testing-with-control-plane-on-separate-machine","title":"Testing with control plane on separate machine","text":"<p>Use SSH reverse tunnel to run control plane locally:</p> <pre><code># On your laptop: Start control plane\ngo run ./cmd/control-plane\n\n# SSH with reverse tunnel\nssh -R 50051:localhost:50051 ubuntu@&lt;gpu-instance&gt;\n\n# On GPU instance: Start node\ngo run ./cmd/node --node-id remote-gpu --provider lambda\n</code></pre> <p>This allows testing the full distributed architecture.</p>"},{"location":"testing-on-gpu/#stress-testing","title":"Stress testing","text":""},{"location":"testing-on-gpu/#gpu-load-test","title":"GPU load test","text":"<pre><code># Run stress test with health monitoring\n./scripts/stress-gpu.sh 600  # 10 minutes\n\n# Watch for:\n# - Temperature increases (should stay below 83\u00b0C)\n# - Power usage (should stay within TDP)\n# - XID errors (should be none)\n</code></pre>"},{"location":"testing-on-gpu/#sustained-operation-test","title":"Sustained operation test","text":"<p>Run the node daemon for extended periods:</p> <pre><code># Start node daemon\nnohup go run ./cmd/node --node-id endurance-test &gt; /tmp/node.log 2&gt;&amp;1 &amp;\n\n# Monitor for 24 hours\nwatch -n 60 'go run ./cmd/navarch get endurance-test | grep -E \"Health|Heartbeat\"'\n</code></pre>"},{"location":"testing-on-gpu/#validating-specific-gpu-types","title":"Validating specific GPU types","text":""},{"location":"testing-on-gpu/#nvidia-a100","title":"NVIDIA A100","text":"<ul> <li>Memory: 40GB or 80GB</li> <li>Expected temperature: 30-45\u00b0C idle, 60-75\u00b0C under load</li> <li>XID codes to watch: 79 (fallen off bus), 48 (DBE)</li> </ul>"},{"location":"testing-on-gpu/#nvidia-h100","title":"NVIDIA H100","text":"<ul> <li>Memory: 80GB</li> <li>Expected temperature: 25-40\u00b0C idle, 55-70\u00b0C under load</li> <li>Higher power consumption (700W TDP)</li> </ul>"},{"location":"testing-on-gpu/#nvidia-l4","title":"NVIDIA L4","text":"<ul> <li>Memory: 24GB</li> <li>Lower power consumption</li> <li>Good for inference workloads</li> </ul>"},{"location":"testing-on-gpu/#troubleshooting","title":"Troubleshooting","text":""},{"location":"testing-on-gpu/#gpu-not-detected","title":"GPU not detected","text":"<pre><code># Check NVIDIA driver\nnvidia-smi\n\n# Check that driver is loaded\nlsmod | grep nvidia\n\n# Reinstall driver if needed\nsudo apt-get install --reinstall nvidia-driver-535\n</code></pre>"},{"location":"testing-on-gpu/#health-checks-failing","title":"Health checks failing","text":"<pre><code># Check node daemon logs\ncat /tmp/node.log | grep -i error\n\n# Check dmesg for GPU errors\nsudo dmesg | grep -i nvidia | tail -20\n</code></pre>"},{"location":"testing-on-gpu/#high-temperature-warnings","title":"High temperature warnings","text":"<pre><code># Check current temperature\nnvidia-smi --query-gpu=temperature.gpu --format=csv\n\n# Check cooling (data center GPUs need proper airflow)\nnvidia-smi --query-gpu=fan.speed --format=csv\n</code></pre>"},{"location":"testing-on-gpu/#cleanup","title":"Cleanup","text":"<p>Remember to terminate GPU instances after testing:</p> <pre><code># Lambda Labs: Use console\n\n# GCP\ngcloud compute instances delete navarch-test --zone=us-central1-a\n\n# AWS\naws ec2 terminate-instances --instance-ids i-xxxxx\n</code></pre> <p>GPU instances are expensive. Always terminate when done.</p>"},{"location":"testing/","title":"Testing","text":""},{"location":"testing/#unit-tests","title":"Unit tests","text":"<p>Run all unit tests:</p> <pre><code>make test\n</code></pre> <p>Run with race detection:</p> <pre><code>make test-race\n</code></pre> <p>Run all checks (format, lint, test):</p> <pre><code>make test-all\n</code></pre>"},{"location":"testing/#docker-provider","title":"Docker provider","text":"<p>The <code>docker</code> provider spawns SSH-enabled containers for end-to-end bootstrap testing without cloud infrastructure.</p> <pre><code>package main\n\nimport (\n    \"context\"\n    \"log\"\n\n    \"github.com/NavarchProject/navarch/pkg/provider\"\n    \"github.com/NavarchProject/navarch/pkg/provider/docker\"\n)\n\nfunc main() {\n    ctx := context.Background()\n\n    p, err := docker.New(docker.Config{\n        SSHPublicKeyPath: \"~/.ssh/id_rsa.pub\",\n    })\n    if err != nil {\n        log.Fatal(err)\n    }\n    defer p.TerminateAll()\n\n    if err := p.EnsureImage(ctx); err != nil {\n        log.Fatal(err)\n    }\n\n    node, err := p.Provision(ctx, provider.ProvisionRequest{\n        Labels: map[string]string{\"pool\": \"test\"},\n    })\n    if err != nil {\n        log.Fatal(err)\n    }\n\n    // node.IPAddress is \"127.0.0.1\"\n    // node.SSHPort is the dynamically assigned host port\n    log.Printf(\"Provisioned %s on port %d\", node.ID, node.SSHPort)\n}\n</code></pre> <p>The provider:</p> <ul> <li>Uses the <code>linuxserver/openssh-server</code> image</li> <li>Maps container port 22 to a random host port</li> <li>Sets <code>SSHPort</code> on the returned node for bootstrap</li> </ul>"},{"location":"testing/#simulator","title":"Simulator","text":"<p>The simulator runs a virtual GPU fleet for scenario testing without cloud resources.</p> <pre><code>./bin/simulator run scenarios/gpu-failure.yaml -v\n</code></pre> <p>Interactive mode starts a control plane you can query with the CLI:</p> <pre><code>./bin/simulator interactive -v\n</code></pre> <p>See Scenarios for the scenario file format and Stress Testing for chaos testing.</p>"},{"location":"testing/#hardware-testing","title":"Hardware testing","text":"<p>To test on real GPU machines, see Testing on Hardware.</p>"},{"location":"troubleshooting/","title":"Troubleshooting","text":"<p>Common issues and how to resolve them.</p>"},{"location":"troubleshooting/#control-plane-issues","title":"Control plane issues","text":""},{"location":"troubleshooting/#control-plane-wont-start","title":"Control plane won't start","text":"<p>Symptom: Control plane exits immediately or fails to bind.</p> <p>Check the logs: <pre><code>control-plane -config navarch.yaml 2&gt;&amp;1 | head -50\n</code></pre></p> <p>Common causes:</p> Error Cause Fix <code>address already in use</code> Another process on the port Change <code>server.grpc_port</code> or stop the other process <code>invalid configuration</code> YAML syntax error Validate with <code>yq</code> or an online YAML validator <code>provider not found</code> Missing provider config Add the provider to <code>providers:</code> section <code>failed to initialize provider</code> Bad credentials Check API key environment variables"},{"location":"troubleshooting/#nodes-not-registering","title":"Nodes not registering","text":"<p>Symptom: Nodes are running but don't appear in <code>navarch list</code>.</p> <p>Check node agent logs: <pre><code>journalctl -u navarch-node -f\n</code></pre></p> <p>Common causes:</p> Error Cause Fix <code>connection refused</code> Wrong control plane address Check <code>--server</code> flag or <code>NAVARCH_SERVER</code> env var <code>TLS handshake failed</code> Certificate mismatch Use <code>--insecure</code> for testing or fix certificates <code>authentication failed</code> Bad or missing token Check <code>--token</code> flag matches control plane config <code>context deadline exceeded</code> Network/firewall issue Check security groups allow gRPC port"},{"location":"troubleshooting/#health-checks-not-running","title":"Health checks not running","text":"<p>Symptom: Nodes show as healthy but health data is stale.</p> <p>Check: 1. Node agent is running: <code>systemctl status navarch-node</code> 2. NVML is accessible: <code>nvidia-smi</code> works on the node 3. Health check interval in config isn't too long</p>"},{"location":"troubleshooting/#node-agent-issues","title":"Node agent issues","text":""},{"location":"troubleshooting/#nvml-initialization-failed","title":"NVML initialization failed","text":"<p>Symptom: Node agent logs <code>failed to initialize NVML</code> or <code>NVML not found</code>.</p> <p>Causes and fixes:</p> <ol> <li> <p>NVIDIA drivers not installed <pre><code>nvidia-smi  # Should show GPU info\n</code></pre>    If not working, install drivers.</p> </li> <li> <p>libnvidia-ml.so not in path <pre><code>ldconfig -p | grep nvidia-ml\n</code></pre>    If missing, the NVIDIA driver installation is incomplete.</p> </li> <li> <p>Running in container without GPU access <pre><code>docker run --gpus all ...  # Need --gpus flag\n</code></pre></p> </li> </ol>"},{"location":"troubleshooting/#node-marked-unhealthy-but-gpus-are-fine","title":"Node marked unhealthy but GPUs are fine","text":"<p>Symptom: Node transitions to unhealthy but <code>nvidia-smi</code> shows no issues.</p> <p>Check health events: <pre><code>navarch get &lt;node-id&gt; -o json | jq '.health_events'\n</code></pre></p> <p>Common causes: - Transient XID error that resolved - Thermal throttling that recovered - Network blip caused missed heartbeats</p> <p>Resolution: If the issue resolved, the node will recover automatically (for non-fatal errors). For fatal XID errors, the node must be replaced.</p>"},{"location":"troubleshooting/#node-agent-high-cpu-usage","title":"Node agent high CPU usage","text":"<p>Symptom: Node agent using excessive CPU.</p> <p>Common causes: - Health check interval too short (sub-second) - NVML calls hanging and retrying - Debug logging enabled in production</p> <p>Fix: Set reasonable intervals in config: <pre><code>heartbeat_interval: 30s\nhealth_check_interval: 60s\n</code></pre></p>"},{"location":"troubleshooting/#autoscaling-issues","title":"Autoscaling issues","text":""},{"location":"troubleshooting/#pool-not-scaling-up","title":"Pool not scaling up","text":"<p>Symptom: Utilization is high but no new nodes are provisioned.</p> <p>Check:</p> <ol> <li> <p>At max capacity? <pre><code>navarch pool status &lt;pool-name&gt;\n</code></pre>    If <code>total_nodes == max_nodes</code>, can't scale further.</p> </li> <li> <p>Cooldown active? <pre><code>grep \"cooldown\" control-plane.log\n</code></pre>    Wait for cooldown to expire.</p> </li> <li> <p>Provider quota exceeded?    Check cloud provider console for quota limits.</p> </li> <li> <p>Metrics not flowing?    Ensure nodes are sending heartbeats with utilization data.</p> </li> </ol>"},{"location":"troubleshooting/#pool-not-scaling-down","title":"Pool not scaling down","text":"<p>Symptom: Utilization is low but nodes aren't terminated.</p> <p>Check:</p> <ol> <li> <p>At min capacity?    If <code>total_nodes == min_nodes</code>, can't scale lower.</p> </li> <li> <p>Cooldown active?    Scale-down also respects cooldown.</p> </li> <li> <p>Nodes cordoned?    Cordoned nodes are prioritized for removal but won't be removed if at min.</p> </li> </ol>"},{"location":"troubleshooting/#autoscaler-oscillating","title":"Autoscaler oscillating","text":"<p>Symptom: Pool scales up and down repeatedly.</p> <p>Causes: - Cooldown too short - Thresholds too close together - Utilization hovering at threshold</p> <p>Fix: Increase cooldown and widen threshold gap: <pre><code>autoscaling:\n  type: reactive\n  scale_up_at: 80\n  scale_down_at: 30  # Gap of 50 points\ncooldown_period: 10m\n</code></pre></p>"},{"location":"troubleshooting/#provider-issues","title":"Provider issues","text":""},{"location":"troubleshooting/#lambda-labs-no-available-instances","title":"Lambda Labs: \"No available instances\"","text":"<p>Symptom: Provisioning fails with availability error.</p> <p>Cause: Lambda Labs has limited inventory. Requested instance type not available in region.</p> <p>Fixes: - Try a different region - Try a different instance type - Set up multi-region failover in pool config - Wait and retry (inventory changes)</p>"},{"location":"troubleshooting/#gcp-quota-exceeded","title":"GCP: \"Quota exceeded\"","text":"<p>Symptom: Provisioning fails with quota error.</p> <p>Fix: Request quota increase in GCP console for: - GPUs (per region) - vCPUs - IP addresses</p>"},{"location":"troubleshooting/#aws-insufficientinstancecapacity","title":"AWS: \"InsufficientInstanceCapacity\"","text":"<p>Symptom: Provisioning fails with capacity error.</p> <p>Fixes: - Try a different availability zone - Use capacity reservations for guaranteed access - Try Spot instances if workload tolerates interruption</p>"},{"location":"troubleshooting/#cli-issues","title":"CLI issues","text":""},{"location":"troubleshooting/#connection-refused","title":"\"Connection refused\"","text":"<p>Symptom: CLI commands fail with connection error.</p> <p>Fixes: 1. Check control plane is running 2. Check address: <code>navarch -s http://correct-host:50051 list</code> 3. Check firewall allows traffic</p>"},{"location":"troubleshooting/#permission-denied","title":"\"Permission denied\"","text":"<p>Symptom: Commands fail with auth error.</p> <p>Fix: Provide valid token: <pre><code>export NAVARCH_TOKEN=your-token\nnavarch list\n</code></pre></p> <p>Or: <pre><code>navarch --token your-token list\n</code></pre></p>"},{"location":"troubleshooting/#simulator-issues","title":"Simulator issues","text":""},{"location":"troubleshooting/#scenario-fails-to-load","title":"Scenario fails to load","text":"<p>Symptom: <code>simulator run</code> fails with parse error.</p> <p>Fix: Validate scenario syntax: <pre><code>simulator validate scenarios/my-scenario.yaml\n</code></pre></p> <p>Check for: - YAML indentation errors - Invalid action names - Missing required fields</p>"},{"location":"troubleshooting/#stress-test-runs-out-of-memory","title":"Stress test runs out of memory","text":"<p>Symptom: Simulator crashes or system becomes unresponsive during stress test.</p> <p>Cause: Too many simulated nodes for available RAM.</p> <p>Fixes: - Reduce <code>total_nodes</code> - Use <code>wave</code> startup pattern with longer duration - Increase system memory - Run on a larger machine</p> <p>See Simulator performance considerations for memory estimates.</p>"},{"location":"troubleshooting/#getting-more-help","title":"Getting more help","text":"<p>If your issue isn't covered here:</p> <ol> <li>Check existing GitHub issues</li> <li>Open a new issue with:</li> <li>Navarch version (<code>navarch version</code>)</li> <li>Config file (redact secrets)</li> <li>Relevant logs</li> <li>Steps to reproduce</li> </ol>"},{"location":"concepts/","title":"Concepts","text":"<p>Navarch manages GPU fleets through a few core abstractions. This section explains how they work together.</p> <ul> <li> <p>Components</p> <p>Control plane and node agent architecture.</p> <p> Components</p> </li> <li> <p>Pools &amp; Providers</p> <p>Organizing nodes by workload and cloud provider.</p> <p> Pools</p> </li> <li> <p>Health Monitoring</p> <p>Health checks, status types, and failure detection.</p> <p> Health</p> </li> <li> <p>Node Lifecycle</p> <p>Instance provisioning, node states, and transitions.</p> <p> Lifecycle</p> </li> <li> <p>Autoscaling</p> <p>Scaling strategies, limits, and cooldown behavior.</p> <p> Autoscaling</p> </li> </ul>"},{"location":"concepts/autoscaling/","title":"Autoscaling","text":"<p>Autoscaling adjusts pool size based on demand. Navarch supports multiple strategies that you can use alone or combine.</p>"},{"location":"concepts/autoscaling/#scaling-limits","title":"Scaling limits","text":"<p>Each pool has minimum and maximum node counts:</p> <pre><code>pools:\n  training:\n    min_nodes: 2   # Never scale below this\n    max_nodes: 20  # Never scale above this\n</code></pre> <ul> <li><code>min_nodes</code>: Floor for scaling. Set to 0 for pools that can be empty.</li> <li><code>max_nodes</code>: Ceiling for scaling. Protects against cost overruns.</li> </ul> <p>The autoscaler operates within these limits. Manual scaling commands also respect them.</p>"},{"location":"concepts/autoscaling/#cooldown-period","title":"Cooldown period","text":"<p>The cooldown period prevents thrashing when metrics fluctuate:</p> <pre><code>pools:\n  training:\n    cooldown: 5m\n</code></pre> <p>During cooldown:</p> <ul> <li>The autoscaler still evaluates state.</li> <li>Recommendations are calculated but not acted upon.</li> <li>Manual scaling commands are still accepted.</li> </ul>"},{"location":"concepts/autoscaling/#autoscaling-strategies","title":"Autoscaling strategies","text":""},{"location":"concepts/autoscaling/#reactive","title":"Reactive","text":"<p>Scales based on current GPU utilization. Use for steady workloads where current load predicts future load.</p> <pre><code>autoscaling:\n  type: reactive\n  scale_up_at: 80    # Scale up when utilization &gt; 80%\n  scale_down_at: 20  # Scale down when utilization &lt; 20%\n</code></pre> <p>How it works: Averages GPU utilization across all nodes in the pool. If above <code>scale_up_at</code>, adds a node. If below <code>scale_down_at</code>, removes a node.</p>"},{"location":"concepts/autoscaling/#queue-based","title":"Queue-based","text":"<p>Scales based on pending job count. Use for batch processing where queue depth indicates required capacity.</p> <pre><code>autoscaling:\n  type: queue\n  jobs_per_node: 10  # Target 10 jobs per node\n</code></pre> <p>How it works: Queries your scheduler for pending jobs, divides by <code>jobs_per_node</code>, rounds up. Requires a metrics source integration.</p> <p>See Metrics Reference for Kubernetes and Slurm integration examples.</p>"},{"location":"concepts/autoscaling/#scheduled","title":"Scheduled","text":"<p>Adjusts scaling limits based on time of day. Use for predictable patterns like business hours.</p> <pre><code>autoscaling:\n  type: scheduled\n  schedule:\n    - days: [monday, tuesday, wednesday, thursday, friday]\n      start: 9\n      end: 18\n      min_nodes: 10\n      max_nodes: 50\n    - days: [saturday, sunday]\n      start: 0\n      end: 24\n      min_nodes: 0\n      max_nodes: 5\n</code></pre> <p>How it works: Overrides pool's <code>min_nodes</code> and <code>max_nodes</code> based on current time. Combine with a fallback strategy for within-window scaling.</p>"},{"location":"concepts/autoscaling/#predictive","title":"Predictive","text":"<p>Uses historical data to anticipate demand. Use for workloads with gradual ramp-up patterns.</p> <pre><code>autoscaling:\n  type: predictive\n  lookback_window: 10   # Analyze last 10 samples\n  growth_factor: 1.2    # Scale 20% ahead of predicted need\n</code></pre> <p>How it works: Analyzes utilization trend over recent samples. If trending up, scales proactively. Falls back to reactive behavior when trend is flat.</p>"},{"location":"concepts/autoscaling/#composite","title":"Composite","text":"<p>Combines multiple strategies. Use for complex scenarios requiring multiple signals.</p> <pre><code>autoscaling:\n  type: composite\n  mode: max  # Take the highest recommendation\n  strategies:\n    - type: reactive\n      scale_up_at: 80\n      scale_down_at: 30\n    - type: queue\n      jobs_per_node: 10\n</code></pre> <p>Modes:</p> <ul> <li><code>max</code>: Take highest recommendation (most aggressive)</li> <li><code>min</code>: Take lowest recommendation (most conservative)</li> <li><code>avg</code>: Average all recommendations</li> </ul>"},{"location":"concepts/autoscaling/#choosing-a-strategy","title":"Choosing a strategy","text":"Workload Strategy Why Steady serving load Reactive Current utilization predicts future Batch processing Queue Job count is the right signal Business hours traffic Scheduled + Reactive Predictable pattern with variation Training jobs Queue or Reactive Depends on how jobs are submitted Mixed workloads Composite Multiple signals needed"},{"location":"concepts/autoscaling/#example-configurations","title":"Example configurations","text":""},{"location":"concepts/autoscaling/#high-availability-serving","title":"High-availability serving","text":"<p>Keep capacity ready, scale aggressively:</p> <pre><code>pools:\n  serving:\n    min_nodes: 5\n    max_nodes: 50\n    cooldown: 2m\n    autoscaling:\n      type: reactive\n      scale_up_at: 60   # Scale up early\n      scale_down_at: 20\n</code></pre>"},{"location":"concepts/autoscaling/#cost-optimized-batch","title":"Cost-optimized batch","text":"<p>Scale to zero when idle, scale fast when jobs arrive:</p> <pre><code>pools:\n  batch:\n    min_nodes: 0\n    max_nodes: 100\n    cooldown: 1m\n    autoscaling:\n      type: queue\n      jobs_per_node: 1  # One job per node\n</code></pre>"},{"location":"concepts/autoscaling/#business-hours-with-buffer","title":"Business hours with buffer","text":"<p>Higher capacity during work hours:</p> <pre><code>pools:\n  dev:\n    min_nodes: 0\n    max_nodes: 20\n    cooldown: 5m\n    autoscaling:\n      type: scheduled\n      schedule:\n        - days: [monday, tuesday, wednesday, thursday, friday]\n          start: 8\n          end: 20\n          min_nodes: 5\n          max_nodes: 20\n      fallback:\n        type: reactive\n        scale_up_at: 70\n        scale_down_at: 30\n</code></pre> <p>See Configuration Reference for all autoscaling options.</p>"},{"location":"concepts/components/","title":"Components","text":"<p>Navarch has two main components: the control plane and the node agent.</p>"},{"location":"concepts/components/#control-plane","title":"Control plane","text":"<p>The control plane is the central management server for your GPU fleet. It:</p> <ul> <li>Receives health reports from node agents.</li> <li>Tracks node status and lifecycle state.</li> <li>Manages node pools and autoscaling.</li> <li>Issues commands to nodes (cordon, drain, terminate).</li> <li>Provides an API for the CLI and external integrations.</li> </ul> <p>There is one control plane per Navarch deployment. All nodes connect to it.</p>"},{"location":"concepts/components/#running-the-control-plane","title":"Running the control plane","text":"<pre><code>control-plane -config navarch.yaml\n</code></pre> <p>The control plane exposes:</p> <ul> <li>gRPC API (default port 50051): For node agents and programmatic access.</li> <li>HTTP API (default port 8080): For the CLI and health checks.</li> </ul> <p>See Configuration Reference for all available options.</p>"},{"location":"concepts/components/#node-agent","title":"Node agent","text":"<p>The node agent runs on each GPU instance. It:</p> <ul> <li>Registers the node with the control plane at startup.</li> <li>Sends periodic heartbeats to prove liveness.</li> <li>Runs health checks and reports results.</li> <li>Receives and executes commands from the control plane.</li> </ul> <p>The node agent does not manage its own lifecycle. It reports status and follows commands. The control plane decides when to terminate nodes.</p>"},{"location":"concepts/components/#running-the-node-agent","title":"Running the node agent","text":"<pre><code>node-agent --server control-plane.example.com:50051\n</code></pre> <p>The agent needs:</p> <ul> <li>Network access to the control plane.</li> <li>Access to NVIDIA drivers (for GPU health checks).</li> <li>Permission to read GPU metrics via NVML.</li> </ul> <p>See Deployment for production setup including systemd configuration.</p>"},{"location":"concepts/components/#communication-flow","title":"Communication flow","text":"<pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510                      \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 Node Agent  \u2502 \u2500\u2500\u2500\u2500\u2500 Register \u2500\u2500\u2500\u2500\u25ba \u2502               \u2502\n\u2502             \u2502                      \u2502 Control Plane \u2502\n\u2502             \u2502 \u25c4\u2500\u2500\u2500\u2500 Commands \u2500\u2500\u2500\u2500\u2500 \u2502               \u2502\n\u2502             \u2502                      \u2502               \u2502\n\u2502             \u2502 \u2500\u2500 Heartbeat/Health\u25ba \u2502               \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                      \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <ol> <li> <p>Registration: On startup, the node agent calls <code>RegisterNode</code> with its metadata (provider, region, GPU info).</p> </li> <li> <p>Heartbeats: Every 30 seconds (configurable), the agent sends a heartbeat with current metrics.</p> </li> <li> <p>Health reports: With each heartbeat, the agent includes health check results.</p> </li> <li> <p>Commands: The control plane can send commands (cordon, drain) which the agent executes.</p> </li> </ol>"},{"location":"concepts/components/#high-availability","title":"High availability","text":"<p>For production deployments:</p> <ul> <li>Run multiple control plane replicas behind a load balancer.</li> <li>Use a shared database backend for state.</li> <li>Node agents reconnect automatically if the control plane restarts.</li> </ul> <p>See Deployment for production setup details.</p>"},{"location":"concepts/health/","title":"Health Monitoring","text":"<p>Navarch detects GPU failures before they crash your workloads.</p>"},{"location":"concepts/health/#health-checks","title":"Health checks","text":"<p>The node agent runs three types of health checks:</p> <ul> <li> <p>Boot check: Validates that the node started correctly and can communicate with the control plane. Runs once at startup.</p> </li> <li> <p>GPU check: Queries GPU metrics via NVML (temperature, power, utilization, memory). Detects communication failures and threshold violations.</p> </li> <li> <p>Health event check: Collects GPU health events and sends them to the control plane. The control plane uses CEL policies to classify events by severity.</p> </li> </ul>"},{"location":"concepts/health/#health-event-types","title":"Health event types","text":"Type Description XID error NVIDIA driver errors (hardware faults, driver issues) Thermal Temperature warnings and critical events ECC SBE Single-bit ECC errors (correctable) ECC DBE Double-bit ECC errors (uncorrectable) NVLink NVLink communication errors PCIe PCIe bus errors"},{"location":"concepts/health/#xid-errors","title":"XID errors","text":"<p>XID errors are NVIDIA driver error codes. Some are fatal (require node replacement), others are recoverable.</p> <p>Fatal XID codes:</p> XID Description 43 GPU stopped processing 48 Double bit ECC error 63 ECC page retirement 79 GPU has fallen off the bus <p>Recoverable XID codes:</p> XID Description 13 Graphics engine exception 31 GPU memory page fault 45 Preemptive cleanup 64 ECC page retirement event 92 High single-bit ECC rate 94 Contained ECC error <p>When a fatal XID occurs, the node is marked unhealthy and (if auto-replace is enabled) terminated and replaced.</p>"},{"location":"concepts/health/#health-status","title":"Health status","text":"<p>Health status reflects the hardware health reported by the node agent.</p> Status Meaning Healthy All health checks pass. GPUs working normally. Degraded Partially functional. Some warnings (high temp, minor errors). Unhealthy Critical failure. One or more checks failed. <p>Health status is computed from check results:</p> <ul> <li>Any check unhealthy \u2192 overall unhealthy</li> <li>Any check degraded (none unhealthy) \u2192 overall degraded</li> <li>All checks healthy \u2192 overall healthy</li> </ul>"},{"location":"concepts/health/#node-status","title":"Node status","text":"<p>Node status reflects the operational state from the control plane's perspective.</p> Status Meaning Active Available for workloads. Receiving heartbeats, passing checks. Cordoned Marked unschedulable. Existing workloads continue. Draining Evicting workloads before termination. Unhealthy Failed health checks. Not usable. Terminated Instance shut down."},{"location":"concepts/health/#how-health-and-node-status-interact","title":"How health and node status interact","text":"<p>Health status affects node status through these rules:</p> Health Status Node Status Transition Unhealthy Node becomes Unhealthy Healthy Node stays Unhealthy (no auto-recovery) Degraded Node stays Unhealthy (no auto-recovery) <p>Unhealthy nodes do not automatically recover to Active. This prevents nodes with intermittent hardware failures from being returned to service. To bring an unhealthy node back:</p> <ul> <li>Use <code>navarch uncordon &lt;node-id&gt;</code> after manually verifying the hardware is healthy.</li> <li>Or let auto-replacement terminate and replace the node.</li> </ul>"},{"location":"concepts/health/#health-based-replacement","title":"Health-based replacement","text":"<p>When <code>auto_replace</code> is enabled, Navarch automatically replaces unhealthy nodes:</p> <ol> <li>Node fails health checks consecutively.</li> <li>After <code>unhealthy_threshold</code> failures, node is marked unhealthy.</li> <li>Navarch terminates the unhealthy node.</li> <li>Navarch provisions a replacement.</li> </ol> <pre><code>pools:\n  training:\n    health:\n      auto_replace: true\n      unhealthy_threshold: 2  # Replace after 2 consecutive failures\n</code></pre> <p>This maintains pool capacity even when GPU hardware fails.</p> <p>See Pool Management for detailed health policy configuration.</p>"},{"location":"concepts/lifecycle/","title":"Node Lifecycle","text":"<p>Navarch tracks instances and nodes as separate concepts with distinct lifecycles.</p>"},{"location":"concepts/lifecycle/#instances-vs-nodes","title":"Instances vs Nodes","text":"<ul> <li>Instance: A cloud resource (what you pay for). Tracked from <code>Provision()</code> until termination.</li> <li>Node: A registered agent running on an instance. Created when the agent calls <code>RegisterNode</code>.</li> </ul> <p>This separation matters because:</p> <ol> <li>Provisioning can fail - Instance created but agent never boots.</li> <li>Registration can fail - Instance running but agent crashes on startup.</li> <li>Costs accrue immediately - You pay for instances, not nodes.</li> </ol>"},{"location":"concepts/lifecycle/#instance-lifecycle","title":"Instance lifecycle","text":"<pre><code>Provisioning \u2192 Pending Registration \u2192 Running \u2192 Terminating \u2192 Terminated\n                        \u2193\n                      Failed\n</code></pre> State Description Provisioning Cloud provider is creating the instance. Pending Registration Instance exists, waiting for node agent to register. Running Node agent has registered successfully. Failed Provisioning failed, or registration timed out. Terminating Termination requested, in progress. Terminated Instance destroyed by cloud provider."},{"location":"concepts/lifecycle/#registration-timeout","title":"Registration timeout","text":"<p>If an instance stays in \"Pending Registration\" too long (default: 10 minutes), Navarch marks it as failed. This catches:</p> <ul> <li>Boot failures (kernel panic, driver issues)</li> <li>Network issues (instance can't reach control plane)</li> <li>Agent crashes (segfault before registration)</li> </ul> <p>Configure the timeout:</p> <pre><code>instance_manager:\n  registration_timeout: 10m\n  stale_check_interval: 1m\n</code></pre>"},{"location":"concepts/lifecycle/#node-lifecycle_1","title":"Node lifecycle","text":"<pre><code>Active \u2192 Cordoned \u2192 Draining \u2192 Terminated\n   \u2191         \u2193\n   \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n    (uncordon)\n</code></pre>"},{"location":"concepts/lifecycle/#active","title":"Active","text":"<p>The node is registered, healthy, and available for workloads. It sends heartbeats and health check results.</p>"},{"location":"concepts/lifecycle/#cordoned","title":"Cordoned","text":"<p>The node is marked unschedulable. New workloads cannot be placed on it, but existing workloads continue.</p> <p>Use cordon for:</p> <ul> <li>Scheduled maintenance</li> <li>Investigating suspected issues</li> <li>Preparing for decommission</li> </ul> <pre><code>navarch cordon node-1\n</code></pre> <p>When a notifier is configured, Navarch notifies your workload system (e.g., Kubernetes, Slurm) to mark the node unschedulable. See Notifier Configuration.</p> <p>See CLI Reference for details.</p>"},{"location":"concepts/lifecycle/#draining","title":"Draining","text":"<p>The node is evicting workloads and will be terminated. No new workloads scheduled.</p> <p>Use drain for:</p> <ul> <li>Decommissioning nodes</li> <li>Responding to hardware failures</li> <li>Forced node replacement</li> </ul> <pre><code>navarch drain node-1\n</code></pre> <p>When a notifier is configured, Navarch notifies your workload system to evacuate workloads from the node. You can poll drain status to wait for completion before termination. See Notifier Configuration.</p> <p>See CLI Reference for details.</p>"},{"location":"concepts/lifecycle/#terminated","title":"Terminated","text":"<p>The instance has been terminated by the provider. The node record remains for historical reference.</p>"},{"location":"concepts/lifecycle/#state-transitions","title":"State transitions","text":""},{"location":"concepts/lifecycle/#manual-transitions","title":"Manual transitions","text":"Command From To <code>cordon</code> Active Cordoned <code>uncordon</code> Cordoned Active <code>drain</code> Active, Cordoned Draining"},{"location":"concepts/lifecycle/#automatic-transitions","title":"Automatic transitions","text":"Trigger From To Health check failure Active, Cordoned Unhealthy Health recovery Unhealthy Active Auto-replacement Unhealthy Terminated Scale-down Active, Cordoned Terminated"},{"location":"concepts/lifecycle/#heartbeats-and-liveness","title":"Heartbeats and liveness","text":"<p>Nodes send heartbeats every 30 seconds (configurable). If heartbeats stop:</p> <ol> <li>After <code>heartbeat_timeout</code> (default: 2 minutes), node is marked stale.</li> <li>Stale nodes are considered unhealthy.</li> <li>If auto-replace is enabled, stale nodes are terminated and replaced.</li> </ol> <p>This handles cases where the node agent crashes or loses network connectivity.</p>"},{"location":"concepts/pools/","title":"Pools &amp; Providers","text":"<p>Pools organize your GPU nodes. Providers connect to cloud platforms.</p>"},{"location":"concepts/pools/#pools","title":"Pools","text":"<p>A pool is a group of GPU nodes with shared configuration:</p> <ul> <li>Same cloud provider and region.</li> <li>Same instance type (GPU count and model).</li> <li>Common scaling limits and autoscaler configuration.</li> <li>Unified health and replacement policies.</li> </ul> <p>Pools let you manage different workload types independently:</p> <pre><code>pools:\n  # Training pool: Large instances, conservative scaling\n  training:\n    provider: lambda\n    instance_type: gpu_8x_h100_sxm5\n    min_nodes: 2\n    max_nodes: 20\n    cooldown: 10m\n\n  # Inference pool: Smaller instances, aggressive scaling\n  inference:\n    provider: lambda\n    instance_type: gpu_1x_a100\n    min_nodes: 5\n    max_nodes: 100\n    cooldown: 2m\n</code></pre>"},{"location":"concepts/pools/#when-to-use-multiple-pools","title":"When to use multiple pools","text":"<ul> <li>Different instance types: Training on 8xH100, inference on 1xA100.</li> <li>Different regions: US pool for US users, EU pool for EU users.</li> <li>Different scaling behavior: Batch jobs scale to zero, serving keeps minimum capacity.</li> <li>Different teams: Separate pools for separate cost tracking.</li> </ul>"},{"location":"concepts/pools/#providers","title":"Providers","text":"<p>A provider abstracts cloud-specific operations:</p> <ul> <li>Provisioning new instances.</li> <li>Terminating instances.</li> <li>Listing available instance types.</li> <li>Managing SSH keys and startup scripts.</li> </ul>"},{"location":"concepts/pools/#supported-providers","title":"Supported providers","text":"Provider Description <code>lambda</code> Lambda Labs Cloud GPU instances. <code>gcp</code> Google Cloud Platform. <code>aws</code> Amazon Web Services. <code>fake</code> Simulated instances for development and testing."},{"location":"concepts/pools/#provider-configuration","title":"Provider configuration","text":"<p>Providers are configured separately from pools:</p> <pre><code>providers:\n  lambda:\n    type: lambda\n    api_key_env: LAMBDA_API_KEY\n\n  gcp:\n    type: gcp\n    project: my-project\n    credentials_file: /path/to/credentials.json\n\npools:\n  training:\n    provider: lambda  # References the provider above\n    # ...\n</code></pre> <p>This lets you:</p> <ul> <li>Use the same provider with different credentials.</li> <li>Switch providers without changing pool configuration.</li> <li>Test with the fake provider before using real clouds.</li> </ul> <p>See Configuration Reference for all provider options.</p>"},{"location":"concepts/pools/#labels","title":"Labels","text":"<p>Labels are key-value pairs attached to pools and nodes. Use them for:</p> <ul> <li>Filtering nodes by workload type.</li> <li>Routing jobs to appropriate pools.</li> <li>Organizing resources by team or project.</li> </ul> <pre><code>pools:\n  training:\n    provider: lambda\n    instance_type: gpu_8x_h100\n    labels:\n      workload: training\n      team: ml-platform\n      environment: production\n</code></pre> <p>Labels propagate to nodes when they're provisioned. Query nodes by label:</p> <pre><code>navarch list --label workload=training\n</code></pre> <p>See CLI Reference for all filtering options.</p>"},{"location":"concepts/pools/#multi-cloud-setup","title":"Multi-cloud setup","text":"<p>Navarch can manage nodes across multiple providers simultaneously:</p> <pre><code>providers:\n  lambda:\n    type: lambda\n    api_key_env: LAMBDA_API_KEY\n\n  gcp:\n    type: gcp\n    project: my-project\n\npools:\n  # Primary pool on Lambda\n  training-primary:\n    provider: lambda\n    instance_type: gpu_8x_h100_sxm5\n    region: us-west-1\n    min_nodes: 4\n    max_nodes: 20\n\n  # Overflow pool on GCP\n  training-overflow:\n    provider: gcp\n    instance_type: a3-highgpu-8g\n    region: us-central1\n    min_nodes: 0\n    max_nodes: 10\n</code></pre> <p>This enables:</p> <ul> <li>Failover: If Lambda is out of capacity, use GCP.</li> <li>Cost optimization: Use the cheapest available provider.</li> <li>Geographic distribution: Run nodes closer to users.</li> </ul>"},{"location":"simulator/","title":"Simulator","text":"<p>The Navarch fleet simulator creates a simulated GPU fleet and control plane for testing, development, and demonstration purposes.</p>"},{"location":"simulator/#overview","title":"Overview","text":"<p>The simulator runs an embedded control plane and spawns simulated nodes that behave like real GPU instances. You can inject failures, issue commands, and observe how the system responds without provisioning actual cloud resources.</p> <p>Use the simulator to:</p> <ul> <li>Test health check logic and failure detection</li> <li>Verify command flows (cordon, drain, terminate)</li> <li>Develop and debug new features locally</li> <li>Run automated integration tests</li> <li>Demo Navarch to others</li> </ul> <ul> <li> <p>Scenarios</p> <p>Define fleets and events in YAML. Inject failures, issue commands, verify behavior.</p> <p> Scenario reference</p> </li> <li> <p>Stress Testing</p> <p>Simulate 1000+ nodes with realistic failure patterns, cascading failures, and auto-recovery.</p> <p> Stress testing guide</p> </li> </ul>"},{"location":"simulator/#building","title":"Building","text":"<pre><code>make build\n</code></pre> <p>This creates <code>bin/simulator</code> along with the other Navarch binaries.</p>"},{"location":"simulator/#running-scenarios","title":"Running scenarios","text":"<p>Scenarios are YAML files that define a fleet configuration and a sequence of events.</p> <pre><code># Run a scenario\n./bin/simulator run scenarios/gpu-failure.yaml -v\n\n# Validate without running\n./bin/simulator validate scenarios/gpu-failure.yaml\n</code></pre>"},{"location":"simulator/#command-line-options","title":"Command-line options","text":"Flag Description <code>-v, --verbose</code> Enable verbose output (INFO level) <code>--debug</code> Enable debug output (DEBUG level) <code>--seed</code> Random seed for reproducible stress tests"},{"location":"simulator/#interactive-mode","title":"Interactive mode","text":"<p>Interactive mode starts a control plane and a default two-node fleet, then waits for you to interact with it using the Navarch CLI.</p> <pre><code>./bin/simulator interactive -v\n</code></pre> <p>In another terminal:</p> <pre><code># List all nodes\nnavarch list -s http://localhost:8080\n\n# Get details about a node\nnavarch get node-1 -s http://localhost:8080\n\n# Cordon a node\nnavarch cordon node-1 -s http://localhost:8080\n</code></pre> <p>Press <code>Ctrl+C</code> to stop.</p>"},{"location":"simulator/#makefile-targets","title":"Makefile targets","text":"<pre><code># Run interactive mode\nmake sim\n\n# Run a specific scenario\nmake sim-run SCENARIO=scenarios/gpu-failure.yaml\n\n# Validate a scenario\nmake sim-validate SCENARIO=scenarios/basic-fleet.yaml\n\n# Run stress tests\nmake sim-run SCENARIO=scenarios/stress/1000-node-chaos.yaml\n</code></pre>"},{"location":"simulator/#next-steps","title":"Next steps","text":"<ul> <li>Scenario reference \u2014 Learn the scenario file format and available actions</li> <li>Stress testing \u2014 Run large-scale chaos tests</li> </ul>"},{"location":"simulator/scenarios/","title":"Scenario Reference","text":"<p>Scenarios are YAML files that define a fleet configuration and a sequence of events to execute.</p>"},{"location":"simulator/scenarios/#file-format","title":"File format","text":"<pre><code>name: example-scenario\ndescription: A brief description of what this scenario tests.\n\nfleet:\n  - id: node-1\n    provider: gcp\n    region: us-central1\n    zone: us-central1-a\n    instance_type: a3-highgpu-8g\n    gpu_count: 8\n    gpu_type: \"NVIDIA H100 80GB HBM3\"\n    labels:\n      environment: test\n\nevents:\n  - at: 0s\n    action: start_fleet\n\n  - at: 5s\n    action: inject_failure\n    target: node-1\n    params:\n      failure_type: xid_error\n      xid_code: 79\n\nassertions:\n  - type: health_status\n    target: node-1\n    expected: unhealthy\n</code></pre>"},{"location":"simulator/scenarios/#fleet-definition","title":"Fleet definition","text":"<p>Each node in the fleet requires:</p> Field Description <code>id</code> Unique identifier for the node <code>provider</code> Cloud provider name (gcp, aws, lambda) <code>region</code> Cloud region <code>zone</code> Availability zone <code>instance_type</code> Instance type (a3-highgpu-8g, p5.48xlarge) <code>gpu_count</code> Number of GPUs on the node <code>gpu_type</code> GPU model name <code>labels</code> Optional key-value labels"},{"location":"simulator/scenarios/#events","title":"Events","text":"<p>Events execute at specified times relative to scenario start. Times use Go duration format (<code>5s</code>, <code>1m30s</code>, <code>500ms</code>).</p> <p>Events with the same time execute sequentially in file order.</p>"},{"location":"simulator/scenarios/#actions","title":"Actions","text":""},{"location":"simulator/scenarios/#start_fleet","title":"start_fleet","text":"<p>Starts all nodes. Each node registers with the control plane and begins sending heartbeats.</p> <pre><code>- at: 0s\n  action: start_fleet\n</code></pre>"},{"location":"simulator/scenarios/#stop_fleet","title":"stop_fleet","text":"<p>Stops all running nodes.</p> <pre><code>- at: 30s\n  action: stop_fleet\n</code></pre>"},{"location":"simulator/scenarios/#inject_failure","title":"inject_failure","text":"<p>Injects a failure condition into a node.</p> <pre><code>- at: 5s\n  action: inject_failure\n  target: node-1\n  params:\n    failure_type: xid_error\n    xid_code: 79\n    gpu_index: 3\n    message: \"GPU has fallen off the bus\"\n</code></pre> <p>Parameters:</p> Parameter Description <code>failure_type</code> Type of failure (see below) <code>xid_code</code> XID error code (for <code>xid_error</code> type) <code>gpu_index</code> Affected GPU index (0-based) <code>message</code> Custom error message <p>Failure types:</p> Type Description <code>xid_error</code> NVIDIA XID error on a specific GPU <code>temperature</code> Thermal event (high GPU temperature) <code>memory_error</code> ECC memory error <code>nvlink_error</code> NVLink communication error <code>backend_error</code> GPU backend failure <code>boot_failure</code> GPU boot/initialization failure"},{"location":"simulator/scenarios/#recover_failure","title":"recover_failure","text":"<p>Clears failures from a node.</p> <pre><code>- at: 20s\n  action: recover_failure\n  target: node-1\n  params:\n    failure_type: xid_error  # Optional: clear only this type\n</code></pre>"},{"location":"simulator/scenarios/#issue_command","title":"issue_command","text":"<p>Issues a command to a node through the control plane.</p> <pre><code>- at: 10s\n  action: issue_command\n  target: node-1\n  params:\n    command_type: cordon\n    command_args:\n      reason: \"maintenance\"\n</code></pre> <p>Command types: <code>cordon</code>, <code>drain</code>, <code>terminate</code>, <code>run_diagnostic</code></p>"},{"location":"simulator/scenarios/#wait_for_status","title":"wait_for_status","text":"<p>Waits for a node to reach a specific status.</p> <pre><code>- at: 12s\n  action: wait_for_status\n  target: node-1\n  params:\n    expected_status: unhealthy\n    timeout: 15s\n</code></pre> <p>Valid statuses: <code>active</code>, <code>cordoned</code>, <code>draining</code>, <code>unhealthy</code>, <code>terminated</code></p>"},{"location":"simulator/scenarios/#wait","title":"wait","text":"<p>Pauses execution.</p> <pre><code>- at: 10s\n  action: wait\n</code></pre>"},{"location":"simulator/scenarios/#log","title":"log","text":"<p>Prints a message to the output.</p> <pre><code>- at: 5s\n  action: log\n  params:\n    log_message: \"Injecting GPU failure...\"\n</code></pre>"},{"location":"simulator/scenarios/#assert","title":"assert","text":"<p>Checks a condition immediately. Fails the scenario if not met.</p> <pre><code>- at: 25s\n  action: assert\n  target: node-1\n  params:\n    expected_status: unhealthy\n</code></pre>"},{"location":"simulator/scenarios/#assertions","title":"Assertions","text":"<p>Assertions at the end of the scenario verify final state. All must pass.</p> <pre><code>assertions:\n  - type: node_status\n    target: node-1\n    expected: active\n\n  - type: health_status\n    target: node-2\n    expected: unhealthy\n</code></pre> Type Description <code>node_status</code> Check node status (active, cordoned, draining, unhealthy, terminated) <code>health_status</code> Check health status (healthy, degraded, unhealthy)"},{"location":"simulator/scenarios/#xid-error-codes","title":"XID error codes","text":"<p>The simulator includes known XID codes with severity classification.</p> <p>Fatal XID codes (require node replacement):</p> Code Name 43 GPU stopped processing 48 Double Bit ECC Error 63 ECC page retirement failure 74 NVLink Error 79 GPU has fallen off the bus 95 Uncontained ECC error <p>Recoverable XID codes:</p> Code Name 13 Graphics Engine Exception 31 GPU memory page fault 32 Invalid push buffer stream 45 Preemptive cleanup 64 ECC page retirement event 68 NVDEC0 Exception 92 High single-bit ECC error rate 94 Contained ECC error"},{"location":"simulator/scenarios/#example-scenarios","title":"Example scenarios","text":"<p>The <code>scenarios/</code> directory contains examples:</p>"},{"location":"simulator/scenarios/#basic-fleetyaml","title":"basic-fleet.yaml","text":"<p>Tests node registration and health reporting.</p> <ul> <li>Starts a three-node fleet (GCP and AWS)</li> <li>Waits for nodes to register</li> <li>Asserts all nodes reach <code>active</code> status</li> </ul>"},{"location":"simulator/scenarios/#gpu-failureyaml","title":"gpu-failure.yaml","text":"<p>Tests fatal GPU failure detection.</p> <ul> <li>Starts two nodes</li> <li>Injects XID 79 on one node</li> <li>Waits for unhealthy status</li> <li>Issues cordon command</li> <li>Asserts affected node is unhealthy, other is active</li> </ul>"},{"location":"simulator/scenarios/#xid-classificationyaml","title":"xid-classification.yaml","text":"<p>Tests XID code classification.</p> <ul> <li>Injects fatal XID on one node</li> <li>Injects recoverable XID on another</li> <li>Recovers the recoverable node</li> <li>Asserts correct final states</li> </ul>"},{"location":"simulator/scenarios/#cordon-drainyaml","title":"cordon-drain.yaml","text":"<p>Tests cordon and drain command flow.</p> <ul> <li>Starts two nodes</li> <li>Issues cordon then drain to one node</li> <li>Asserts other node remains active</li> </ul>"},{"location":"simulator/scenarios/#writing-custom-scenarios","title":"Writing custom scenarios","text":"<ol> <li>Create a YAML file following the format above</li> <li>Validate: <code>./bin/simulator validate your-scenario.yaml</code></li> <li>Run: <code>./bin/simulator run your-scenario.yaml -v</code></li> </ol> <p>Tips:</p> <ul> <li>Start with <code>start_fleet</code> at time 0s</li> <li>Allow 2-3 seconds after <code>start_fleet</code> for nodes to register</li> <li>After injecting failures, allow 5-10 seconds for health checks to propagate</li> <li>Use <code>log</code> actions to document what the scenario is doing</li> <li>Use <code>wait_for_status</code> instead of fixed delays when possible</li> </ul>"},{"location":"simulator/stress-testing/","title":"Stress Testing","text":"<p>The simulator includes a comprehensive stress testing framework for validating system behavior at scale with realistic failure patterns.</p>"},{"location":"simulator/stress-testing/#overview","title":"Overview","text":"<p>Stress tests allow you to:</p> <ul> <li>Simulate thousands of nodes simultaneously</li> <li>Inject failures with realistic distributions based on production data</li> <li>Test cascading failure scenarios</li> <li>Simulate scheduled outages (zone, region, provider)</li> <li>Measure system resilience and recovery</li> <li>Generate detailed reports</li> </ul>"},{"location":"simulator/stress-testing/#running-stress-tests","title":"Running stress tests","text":"<pre><code># Run a stress test\n./bin/simulator run scenarios/stress/1000-node-chaos.yaml -v\n\n# Run with specific seed for reproducibility\n./bin/simulator run scenarios/stress/1000-node-chaos.yaml --seed 12345 -v\n\n# Validate before running\n./bin/simulator validate scenarios/stress/5000-node-extreme.yaml\n</code></pre>"},{"location":"simulator/stress-testing/#configuration","title":"Configuration","text":"<p>Stress tests use an extended scenario format with a <code>stress</code> section:</p> <pre><code>name: my-stress-test\ndescription: Large-scale chaos testing\n\nfleet: []  # Empty when using fleet_gen\n\nstress:\n  duration: 10m\n  metrics_interval: 5s\n  seed: 12345\n  report_file: stress-report.json\n  html_report_file: stress-report.html\n  log_file: stress-report.log\n\n  fleet_gen:\n    # Fleet generation config...\n\n  chaos:\n    # Chaos engineering config...\n</code></pre>"},{"location":"simulator/stress-testing/#fleet-generation","title":"Fleet generation","text":"<p>Instead of defining individual nodes, generate fleets from templates:</p> <pre><code>fleet_gen:\n  total_nodes: 1000\n\n  templates:\n    - name: h100-8gpu\n      weight: 60\n      gpu_count: 8\n      gpu_type: \"NVIDIA H100 80GB HBM3\"\n      instance_type: a3-highgpu-8g\n      labels:\n        tier: premium\n    - name: a100-8gpu\n      weight: 40\n      gpu_count: 8\n      gpu_type: \"NVIDIA A100 80GB\"\n      instance_type: a2-ultragpu-8g\n\n  providers:\n    gcp: 50\n    aws: 35\n    lambda: 15\n\n  regions:\n    us-central1: 40\n    us-east1: 30\n    europe-west1: 30\n\n  startup:\n    pattern: exponential\n    duration: 2m\n    jitter_percent: 15\n</code></pre> Field Description <code>total_nodes</code> Total number of nodes to generate <code>templates</code> Node templates with relative weights <code>providers</code> Provider distribution (percentages) <code>regions</code> Region distribution (percentages) <code>startup</code> How nodes join the cluster"},{"location":"simulator/stress-testing/#startup-patterns","title":"Startup patterns","text":"Pattern Description <code>instant</code> All nodes start immediately <code>linear</code> Nodes start at constant rate <code>exponential</code> Start slow, accelerate (1, 2, 4, 8, ...) <code>wave</code> Start in batches with pauses <pre><code>startup:\n  pattern: wave\n  duration: 5m\n  batch_size: 100\n  jitter_percent: 20\n  cold_start_min: 30s\n  cold_start_max: 2m\n</code></pre> <p>Cold start delays simulate provisioning time. Use <code>cold_start_min</code>/<code>cold_start_max</code> for uniform distribution, or <code>cold_start_mean</code>/<code>cold_start_stddev</code> for normal distribution.</p>"},{"location":"simulator/stress-testing/#chaos-engineering","title":"Chaos engineering","text":"<p>Control failure injection:</p> <pre><code>chaos:\n  enabled: true\n  failure_rate: 10.0  # Failures per minute per 1000 nodes\n\n  xid_distribution:\n    13: 15  # Graphics Engine Exception\n    31: 20  # GPU memory page fault\n    48: 12  # Double Bit ECC Error\n    79: 6   # GPU fallen off bus\n\n  failure_types:\n    - type: xid_error\n      weight: 70\n    - type: temperature\n      weight: 10\n    - type: nvml_failure\n      weight: 8\n    - type: network\n      weight: 10\n    - type: boot_failure\n      weight: 2\n</code></pre>"},{"location":"simulator/stress-testing/#failure-rate","title":"Failure rate","text":"<p>Failures per minute per 1000 nodes: - 1000 nodes with rate 10.0 = ~10 failures/minute - 5000 nodes with rate 10.0 = ~50 failures/minute</p>"},{"location":"simulator/stress-testing/#failure-types","title":"Failure types","text":"Type Description <code>xid_error</code> GPU XID error with specified distribution <code>temperature</code> Thermal throttling/shutdown <code>backend_error</code> GPU backend failure (alias: <code>nvml_failure</code>) <code>boot_failure</code> GPU boot/detection failure <code>network</code> Network connectivity loss <code>memory_error</code> ECC memory error <code>nvlink_error</code> NVLink communication error"},{"location":"simulator/stress-testing/#cascading-failures","title":"Cascading failures","text":"<p>Simulate realistic failure propagation:</p> <pre><code>cascading:\n  enabled: true\n  probability: 0.15      # 15% chance a failure cascades\n  max_depth: 3           # Maximum cascade chain length\n  min_delay: 1s\n  max_delay: 10s\n  scope: zone            # Cascade scope\n  max_affected_percent: 0.1\n</code></pre> <p>Cascade scopes:</p> Scope Description <code>rack</code> Same rack (first 3 node ID segments match) <code>zone</code> Same availability zone <code>region</code> Same region <code>provider</code> Same cloud provider <code>random</code> Any node in cluster"},{"location":"simulator/stress-testing/#automatic-recovery","title":"Automatic recovery","text":"<p>Configure recovery for non-fatal failures:</p> <pre><code>recovery:\n  enabled: true\n  probability: 0.7      # 70% of non-fatal errors recover\n  mean_time: 5m\n  std_dev: 2m\n  replace_fatal: true   # Replace nodes with fatal errors\n  replace_cold_start: 45s\n</code></pre> <p>Recovery only applies to non-fatal XID codes and recoverable failure types.</p>"},{"location":"simulator/stress-testing/#scheduled-outages","title":"Scheduled outages","text":"<p>Simulate planned or unplanned outages:</p> <pre><code>scheduled_outages:\n  - name: zone-network-partition\n    start_time: 10m\n    duration: 5m\n    scope: zone\n    target: us-central1-a\n    failure_type: network\n\n  - name: provider-degradation\n    start_time: 20m\n    duration: 8m\n    scope: provider\n    target: lambda\n    failure_type: xid_error\n\n  - name: random-thermal-event\n    start_time: 15m\n    duration: 3m\n    scope: percentage\n    target: \"10\"\n    failure_type: temperature\n</code></pre> <p>Outage scopes: <code>zone</code>, <code>region</code>, <code>provider</code>, <code>percentage</code></p>"},{"location":"simulator/stress-testing/#correlated-failures","title":"Correlated failures","text":"<p>Define failures that trigger related failures:</p> <pre><code>correlated_failures:\n  - name: nvlink-gpu-cascade\n    trigger: \"74\"           # NVLink error triggers this\n    response: xid_error\n    probability: 0.6\n    delay: 1s\n    scope: same_node\n\n  - name: thermal-propagation\n    trigger: temperature\n    response: temperature\n    probability: 0.4\n    delay: 3s\n    scope: same_rack\n</code></pre> <p>Correlation scopes: <code>same_node</code>, <code>same_rack</code>, <code>same_zone</code>, <code>random</code></p>"},{"location":"simulator/stress-testing/#reports","title":"Reports","text":"<p>Configure report outputs:</p> <pre><code>stress:\n  report_file: stress-report.json\n  html_report_file: stress-report.html\n  log_file: stress-report.log\n</code></pre>"},{"location":"simulator/stress-testing/#html-report","title":"HTML report","text":"<p>Interactive web visualization with:</p> <ul> <li>Results tab: Summary statistics, failure breakdowns, interactive charts</li> <li>Node health over time</li> <li>Failures vs recoveries</li> <li>XID error distribution (pie chart)</li> <li>Failure types breakdown (bar chart)</li> <li>Configuration tab: Full test configuration</li> </ul>"},{"location":"simulator/stress-testing/#json-report","title":"JSON report","text":"<p>Structured data for programmatic analysis:</p> <pre><code>{\n  \"name\": \"1000-node-chaos-test\",\n  \"duration\": \"10m0s\",\n  \"summary\": {\n    \"nodes_started\": 1000,\n    \"peak_healthy_nodes\": 1000,\n    \"min_healthy_nodes\": 847,\n    \"total_failures\": 98,\n    \"total_recoveries\": 45\n  },\n  \"failures\": {\n    \"by_type\": {\"xid_error\": 68, \"temperature\": 12},\n    \"by_xid\": {\"31\": 15, \"79\": 8, \"48\": 7},\n    \"cascading_failures\": 12\n  }\n}\n</code></pre>"},{"location":"simulator/stress-testing/#log-file","title":"Log file","text":"<p>Verbose debug output from all components. Useful for:</p> <ul> <li>Debugging specific failure sequences</li> <li>Post-mortem investigation</li> <li>Providing context to LLMs for analysis</li> </ul>"},{"location":"simulator/stress-testing/#example-scenarios","title":"Example scenarios","text":""},{"location":"simulator/stress-testing/#1000-node-chaosyaml","title":"1000-node-chaos.yaml","text":"<p>Standard chaos test: - 10 minute duration - Mixed H100/A100 fleet across GCP, AWS, Lambda - Realistic XID distribution - Cascading failures enabled - Automatic recovery</p> <pre><code>./bin/simulator run scenarios/stress/1000-node-chaos.yaml -v\n</code></pre>"},{"location":"simulator/stress-testing/#5000-node-extremeyaml","title":"5000-node-extreme.yaml","text":"<p>Extreme stress test: - 30 minute duration - 5000 nodes across 8 regions - Aggressive failure rate (50/min/1000 nodes) - Multiple scheduled outages - High cascade probability</p>"},{"location":"simulator/stress-testing/#xid-comprehensiveyaml","title":"xid-comprehensive.yaml","text":"<p>XID error testing: - All known XID codes tested equally - High recovery rate - No cascading (isolates XID behavior)</p>"},{"location":"simulator/stress-testing/#cascading-failuresyaml","title":"cascading-failures.yaml","text":"<p>Cascade testing: - High cascade probability (50%) - Deep cascade chains (depth 5) - Scheduled outages that trigger cascades - Tests blast radius containment</p>"},{"location":"simulator/stress-testing/#performance-considerations","title":"Performance considerations","text":"Node Count Recommended Startup Memory Usage 100-500 linear, 30s ~200MB 500-1000 linear, 1m ~500MB 1000-2000 exponential, 2m ~1GB 2000-5000 wave, 5m ~2-3GB 5000+ wave, 10m+ ~5GB+ <p>Tips:</p> <ul> <li>Start with smaller node counts (100-500) during development</li> <li>Use <code>--seed</code> for debugging specific failure sequences</li> <li>Monitor memory usage for very large fleets</li> <li>Allow adequate startup time for large fleets</li> <li>Use validate command to check scenario syntax</li> </ul>"}]}